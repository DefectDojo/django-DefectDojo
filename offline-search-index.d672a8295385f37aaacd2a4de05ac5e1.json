

































[{"body":"NGINX The webserver NGINX delivers all static content, e.g. images, JavaScript files or CSS files.\nuWSGI uWSGI is the application server that runs the DefectDojo application, written in Python/Django, to serve all dynamic content.\nMessage Broker The application server sends tasks to a Message Broker for asynchronous execution. RabbitMQ is a well supported choice.\nCelery Worker Tasks like deduplication or the Jira synchonization are performed asynchronously in the background by the Celery Worker.\nCelery Beat In order to identify and notify users about things like upcoming engagements, DefectDojo runs scheduled tasks. These tasks are scheduled and run using Celery Beat.\nInitializer The Initializer gets started during startup of DefectDojo to initialize the database and run database migrations after upgrades of DefectDojo. It shuts itself down after all tasks are performed.\nDatabase The Database stores all data of DefectDojo. Currently MySQL and PostgreSQL are supported. Please note the django-watson search engine require one or more MyISAM tables, so you cannot use Azure MySQL or Cloud SQL for MySQL. AWS RDS MySQL supports MyISAM tables, so it will work.\n","categories":"","description":"DefectDojo consists of several components that work together closely.","excerpt":"DefectDojo consists of several components that work together closely.","ref":"/django-DefectDojo/getting_started/architecture/","tags":"","title":"Architecture"},{"body":"Product Type Product types represent the top level model, these can be business unit divisions, different offices or locations, development teams, or any other logical way of distinguishing ‚Äútypes‚Äù of products.\n Examples:  IAM Team Internal / 3rd Party Main company / Acquisition San Francisco / New York offices    Product This is the name of any project, program, or product that you are currently testing.\n Examples:  Wordpress Internal wiki Slack    Engagement Engagements are moments in time when testing is taking place. They are associated with a name for easy reference, a time line, a lead (the user account of the main person conducting the testing), a test strategy, and a status. Engagement consists of two types: Interactive and CI/CD. An interactive engagement is typically an engagement conducted by an engineer, where findings are usually uploaded by the engineer. A CI/CD engagement, as it‚Äôs name suggests, is for automated integration with a CI/CD pipeline.\n Examples:  Beta Quarterly PCI Scan Release Version X    Test Tests are a grouping of activities conducted by engineers to attempt to discover flaws in a product. Tests are bundled within engagements, have a start and end date and are defined by a test type.\n Examples:  Burp Scan from Oct. 29, 2015 to Oct. 29, 2015 Nessus Scan from Oct. 31, 2015 to Oct. 31, 2015 API Test from Oct. 15, 2015 to Oct. 20, 2015    Finding A finding represents a flaw discovered while testing. It can be categorized with severities of Critical, High, Medium, Low, and Informational (Info).\n Examples:  OpenSSL ‚ÄòChangeCipherSpec‚Äô MiTM Potential Vulnerability Web Application Potentially Vulnerable to Clickjacking Web Browser XSS Protection Not Enabled    Endpoint Endpoints represent testable systems defined by their IP address or Fully Qualified Domain Name.\n Examples:  https://www.example.com https://www.example.com:8080/products 192.168.0.36    ","categories":"","description":"DefectDojo is based on a model that allows high flexibility for your test tracking needs.","excerpt":"DefectDojo is based on a model that allows high flexibility for your ‚Ä¶","ref":"/django-DefectDojo/usage/models/","tags":"","title":"Core data classes"},{"body":"","categories":"","description":"How to install and configure DefectDojo","excerpt":"How to install and configure DefectDojo","ref":"/django-DefectDojo/getting_started/","tags":"","title":"Getting started"},{"body":"Import The importers analyze each report and create new Findings for each item reported. DefectDojo collapses duplicate Findings by capturing the individual hosts vulnerable.\nThis approach will create a new Test for each upload. This can result a lot of findings. If deduplication is enabled, new findings that are identical to existing findings get marked as a duplicate.\nReimport Additionally, DefectDojo allows for re-imports of previously uploaded reports. This greatly reduces the amount of findings as no duplicates are created for findings that already exist.\nDefectDojo will attempt to capture the deltas between the original and new import and automatically add or mitigate findings as appropriate.\nThis behaviour can be controled via the closed_old_findings parameter on the reupload form.\nThe history of a test will be shown with the delta‚Äôs for each reimported scan report. Clicking on a reimport changset will show the affected findings, as well as a status history per finding. Triage-less scanners Some scanners might not include triage information in their reports, such as tfsec for example. They simply scan code or dependencies, flag issues, and return everything. Removing some findings requires you to add comments in your code perhaps, but there is no simple way to filter out findings from the reports.\nThat is why DefectDojo also includes a ‚ÄúDo not reactivate‚Äù checkbox in uploading reports (also in the reimport API), so you can persist the triages that have been done in Defectdojo without reactivating issues on every upload.\nFor context, see #6892\nAPI This section focuses on Import and Reimport via the API. Please see the full documentation details of all API Endpoints for more details. Reimport is actually the easiest way to get started as it will create any entities on the fly if needed and it will automatically detect if it is a first time upload or a re-upload.\nImport Importing via the API is performed via the import-scan endpoint.\nAs described in the Core Data Classes, a test gets created inside an Engagement, inside a Product, inside a Product Type.\nAn import can be performed by specifying the names of these entities in the API request:\n{ \"minimum_severity\": 'Info', \"active\": True, \"verified\": True, \"scan_type\": 'ZAP Scan', \"test_title\": 'Manual ZAP Scan by John', \"product_type_name\": 'Good Products', \"product_name\": 'My little product', \"engagement_name\": 'Important import', \"auto_create_context\": True, } When auto_create_context is True, the product and engagement will be created if needed. Make sure your user has sufficient permissions to do this.\nA classic way of importing a scan is by specifying the ID of the engagement instead:\n{ \"minimum_severity\": 'Info', \"active\": True, \"verified\": True, \"scan_type\": 'ZAP Scan', \"test_title\": 'Manual ZAP Scan by John', \"engagement\": 123, } Reimport ReImporting via the API is performed via the reimport-scan endpoint.\nAn reimport can be performed by specifying the names of these entities in the API request:\n{ \"minimum_severity\": 'Info', \"active\": True, \"verified\": True, \"scan_type\": 'ZAP Scan', \"test_title\": 'Manual ZAP Scan by John', \"product_type_name\": 'Good Products', \"product_name\": 'My little product', \"engagement_name\": 'Important import', \"auto_create_context\": True, \"do_not_reactivate\": False, } When auto_create_context is True, the product and engagement will be created if needed. Make sure your user has sufficient permissions to do this.\nWhen do_not_reactivate is True, the importing/reimporting will ignore uploaded active findings and not reactivate previously closed findings, while still creating new findings if there are new ones. You will get a note on the finding to explain that it was not reactivated for that reason.\nA Reimport will automatically select the latest test inside the provided engagement that satisifes the provided scan_type and (optionally) provided test_title\nIf no existing Test is found, the reimport endpoint will use the import function to import the provided report into a new Test. This means a (CI/CD) script using the API doesn‚Äôt need to know if a Test already exist, or if it is a first time upload for this product / engagement.\nA classic way of reimporting a scan is by specifying the ID of the test instead:\n{ \"minimum_severity\": 'Info', \"active\": True, \"verified\": True, \"scan_type\": 'ZAP Scan', \"test\": 123, } Using the Scan Completion Date (API: scan_date) field DefectDojo offers a plethora of supported scanner reports, but not all of them contain the information most important to a user. The scan_date field is a flexible smart feature that allows users to set the completion date of the a given scan report, and have it propagate down to all the findings imported. This field is not mandatory, but the default value for this field is the date of import (whenever the request is processed and a successful response is returned).\nHere are the following use cases for using this field:\n The report does not set the date, and scan_date is not set at import  Finding date will be the default value of scan_date   The report sets the date, and the scan_date is not set at import  Finding date will be whatever the report sets   The report does not set the date, and the scan_date is set at import  Finding date will be whatever the user set for scan_date   The report sets the date, and the scan_date is set at import  Finding date will be whatever the user set for scan_date    ","categories":"","description":"How DefectDojo imports and reimports security tool reports.","excerpt":"How DefectDojo imports and reimports security tool reports.","ref":"/django-DefectDojo/integrations/importing/","tags":"","title":"Importing"},{"body":" Information All commands assume that you‚Äôre located at the root of the django-DefectDojo cloned repo.  Pre-requisites  You have forked https://github.com/DefectDojo/django-DefectDojo and cloned locally. Checkout dev and make sure you‚Äôre up to date with the latest changes. It‚Äôs advised that you create a dedicated branch for your development, such as git checkout -b parser-name yet that‚Äôs up to you.  It is probably easier to use the docker-compose stack (and benefit from the hot-reload capbility for uWSGI). Set up your environment to use the debug environment, such as:\n$ docker/setEnv.sh debug\nPlease have a look at DOCKER.md for more details.\nDocker images You‚Äôd want to build your docker images locally, and eventually pass in your local user‚Äôs uid to be able to write to the image (handy for database migration files). Assuming your user‚Äôs uid is 1000, then:\n$ docker-compose build --build-arg uid=1000 Which files do you need to modify?    File Purpose     dojo/tools/\u003cparser_dir\u003e/__init__.py Empty file for class initialization   dojo/tools/\u003cparser_dir\u003e/parser.py The meat. This is where you write your actual parser. The class name must be the Python module name without underscores plus Parser. Example: When the name of the Python module is dependency_check, the class name shall be DependencyCheckParser   unittests/scans/\u003cparser_dir\u003e/{many_vulns,no_vuln,one_vuln}.json Sample files containing meaningful data for unit tests. The minimal set.   unittests/tools/test_\u003cparser_name\u003e_parser.py Unit tests of the parser.   dojo/settings/settings.dist.py If you want to use a modern hashcode based deduplication algorithm    Factory contract Parser are loaded dynamicaly with a factory pattern. To have your parser loaded and works correctly, you need to implement the contract.\n your parser MUST be in a sub-module of module dojo.tools  ex: dojo.tools.my_tool.parser module   your parser MUST be a class in this sub-module.  ex: dojo.tools.my_tool.parser.MyToolParser   The name of this class MUST be the Python module name without underscores and with Parser suffix.  ex: dojo.tools.my_tool.parser.MyToolParser   This class MUST have an empty constructor or no constructor This class MUST implement 3 methods:  def get_scan_types(self) This function return a list of all the scan_type supported by your parser. This identifiers are used internally. Your parser can support more than one scan_type. For example some parsers use different identifier to modify the behavior of the parser (aggregate, filter, etc‚Ä¶) def get_label_for_scan_types(self, scan_type): This function return a string used to provide some text in the UI (short label) def get_description_for_scan_types(self, scan_type): This function return a string used to provide some text in the UI (long description) def get_findings(self, file, test) This function return a list of findings   If your parser have more than 1 scan_type (for detailled mode) you MUST implement def set_mode(self, mode) method  Example:\nclass MyToolParser(object): def get_scan_types(self): return [\"My Tool Scan\", \"My Tool Scan detailed\"] def get_label_for_scan_types(self, scan_type): if scan_type == \"My Tool Scan\": return \"My Tool XML Scan aggregated by ...\" else: return \"My Tool XML Scan\" def get_description_for_scan_types(self, scan_type): return \"Aggregates findings per cwe, title, description, file_path. SonarQube output file can be imported in HTML format. Generate with https://github.com/soprasteria/sonar-report version \u003e= 1.1.0\" def requires_file(self, scan_type): return False # mode: # None (default): aggregates vulnerabilites per sink filename (legacy behavior) # 'detailed' : No aggregation mode = None def set_mode(self, mode): self.mode = mode def get_findings(self, file, test): \u003c...\u003e Template Generator Use the template parser to quickly generate the files required. To get started you will need to install cookiecutter.\n$ pip install cookiecutter Then generate your scanner parser from the root of django-DefectDojo:\n$ cookiecutter https://github.com/DefectDojo/cookiecutter-scanner-parser Read more on the template configuration variables.\nThings to pay attention to Here is a list of advise that will make your parser future proof.\nDo not parse URLs by hand We use 2 modules to handle endpoints:\n hyperlink dojo.models with a specific class to handle processing around URLs to create endpoints Endpoint.  All the existing parser use the same code to parse URL and create endpoints. Using Endpoint.from_uri() is the best way to create endpoints. If you really need to parse an URL, use hyperlink module.\nGood example:\nif \"url\" in item: endpoint = Endpoint.from_uri(item[\"url\"]) finding.unsaved_endpoints = [endpoint] Very bad example:\nu = urlparse(item[\"url\"]) endpoint = Endpoint(host=u.host) finding.unsaved_endpoints = [endpoint] Not all attributes are mandatory Parsers may have many fields, out of which many of them may be optional. It better to not set attribute if you don‚Äôt have data instead of filling with values like NA, No data etc‚Ä¶\nCheck class dojo.models.Finding\nData could be missing in the source report Always make sure you include checks to avoid potential KeyError errors (e.g. field does not exist), for those fields you are not absolutely certain will always be in file that will get uploaded. These translate to 500 error, and do not look good.\nGood example:\nif \"mykey\" in data: finding.cve = data[\"mykey\"] Do not parse CVSS by hand (vector, score or severity) Data can have CVSS vectors or scores. Don‚Äôt try to write your own CVSS score algorithm. For parser, we rely on module cvss.\nIt‚Äôs easy to use and will make the parser aligned with the rest of the code.\nExample of use:\nfrom cvss.cvss3 import CVSS3 import cvss.parser vectors = cvss.parser.parse_cvss_from_text(\"CVSS:3.0/S:C/C:H/I:H/A:N/AV:P/AC:H/PR:H/UI:R/E:H/RL:O/RC:R/CR:H/IR:X/AR:X/MAC:H/MPR:X/MUI:X/MC:L/MA:X\") if len(vectors) \u003e 0 and type(vectors[0]) == CVSS3: print(vectors[0].severities()) # this is the 3 severities cvssv3 = vectors[0].clean_vector() severity = vectors[0].severities()[0] vectors[0].compute_base_score() cvssv3_score = vectors[0].scores()[0] print(severity) print(cvssv3_score) Good example:\nvectors = cvss.parser.parse_cvss_from_text(item['cvss_vect']) if len(vectors) \u003e 0 and type(vectors[0]) == CVSS3: finding.cvss = vectors[0].clean_vector() finding.severity = vectors[0].severities()[0] # if your tool does generate severity Bad example (DIY):\ndef get_severity(self, cvss, cvss_version=\"2.0\"): cvss = float(cvss) cvss_version = float(cvss_version[:1]) # If CVSS Version 3 and above if cvss_version \u003e= 3: if cvss \u003e 0 and cvss \u003c 4: return \"Low\" elif cvss \u003e= 4 and cvss \u003c 7: return \"Medium\" elif cvss \u003e= 7 and cvss \u003c 9: return \"High\" elif cvss \u003e= 9: return \"Critical\" else: return \"Informational\" # If CVSS Version prior to 3 else: if cvss \u003e 0 and cvss \u003c 4: return \"Low\" elif cvss \u003e= 4 and cvss \u003c 7: return \"Medium\" elif cvss \u003e= 7 and cvss \u003c= 10: return \"High\" else: return \"Informational\" Deduplication algorithm By default a new parser uses the ‚Äòlegacy‚Äô deduplication algorithm documented at https://documentation.defectdojo.com/usage/features/#deduplication-algorithms\nUnit tests Each parser must have unit tests, at least to test for 0 vuln, 1 vuln and many vulns. You can take a look at how other parsers have them for starters. The more quality tests, the better.\nIt‚Äôs important to add checks on attributes of findings. For ex:\nwith self.subTest(i=0): finding = findings[0] self.assertEqual(\"test title\", finding.title) self.assertEqual(True, finding.active) self.assertEqual(True, finding.verified) self.assertEqual(False, finding.duplicate) self.assertIn(finding.severity, Finding.SEVERITIES) self.assertEqual(\"CVE-2020-36234\", finding.cve) self.assertEqual(261, finding.cwe) self.assertEqual(\"CVSS:3.1/AV:N/AC:L/PR:H/UI:R/S:C/C:L/I:L/A:N\", finding.cvssv3) self.assertIn(\"security\", finding.tags) self.assertIn(\"network\", finding.tags) self.assertEqual(\"3287f2d0-554f-491b-8516-3c349ead8ee5\", finding.unique_id_from_tool) self.assertEqual(\"TEST1\", finding.vuln_id_from_tool) Test database To test your unit tests locally, you first need to grant some rights. Get your MySQL root password from the docker-compose logs, login as root and issue the following commands:\nMYSQL\u003e grant all privileges on test_defectdojo.* to defectdojo@'%'; MYSQL\u003e flush privileges; Run your tests This local command will launch the unit test for your new parser\n$ docker-compose exec uwsgi bash -c 'python manage.py test unittests.tools.\u003cyour_unittest_py_file\u003e.\u003cmain_class_name\u003e -v2' Example for the blackduck hub parser:\n$ docker-compose exec uwsgi bash -c 'python manage.py test unittests.tools.test_blackduck_csv_parser.TestBlackduckHubParser -v2' Information If you want to run all unit tests, simply run $ docker-compose exec uwsgi bash -c 'python manage.py test unittests -v2'  Endpoint validation Some types of parsers create a list of endpoints that are vulnerable (they are stored in finding.unsaved_endpoints). DefectDojo requires storing endpoints in a specific format (which follow RFCs). Endpoints that do not follow this format can be stored but they will be marked as broken (red flag üö©in UI). To be sure your parse store endpoints in the correct format run the .clean() function for all endpoints in unit tests\nfindings = parser.get_findings(testfile, Test()) for finding in findings: for endpoint in finding.unsaved_endpoints: endpoint.clean() Other files that could be involved Change to the model In the event where you‚Äôd have to change the model, e.g. to increase a database column size to accomodate a longer string of data to be saved\n  Change what you need in dojo/models.py\n  Create a new migration file in dojo/db_migrations by running and including as part of your PR\n$ docker-compose exec uwsgi bash -c 'python manage.py makemigrations -v2'    Accept a different type of file to upload If you want to be able to accept a new type of file for your parser, take a look at dojo/forms.py around line 436 (at the time of this writing) or locate the 2 places (for import and re-import) where you find the string attrs={\"accept\":.\nFormats currently accepted: .xml, .csv, .nessus, .json, .html, .js, .zip.\nA need for more than just the parser.py Of course, nothing prevents you from having more files than the parser.py file. It‚Äôs python :-)\nPull request examples If you want to take a look at previous parsers that are now part of DefectDojo, take a look at https://github.com/DefectDojo/django-DefectDojo/pulls?q=is%3Apr+sort%3Aupdated-desc+label%3A%22Import+Scans%22+is%3Aclosed\nUpdate the import page documentation Please update [docs/content/en/integrations/parsers.md] with the details of your new parser.\n","categories":"","description":"How to contribute to parsers","excerpt":"How to contribute to parsers","ref":"/django-DefectDojo/contributing/how-to-write-a-parser/","tags":"","title":"Parsers"},{"body":"Security Tools Acunetix Scanner XML format\nAcunetix 360 Scanner Vulnerabilities List - JSON report\nAnchore-Engine JSON vulnerability report generated by anchore-cli tool, using a command like anchore-cli --json image vuln \u003cimage:tag\u003e all\nAqua JSON report format.\nAnchore Grype Anchore Grype JSON report format generated with -o json option.\ngrype defectdojo/defectdojo-django:1.13.1 -o json \u003e many_vulns.json Arachni Scanner Arachni Web Scanner (https://www.arachni-scanner.com)\nReports are generated with arachni_reporter tool this way:\narachni_reporter --reporter 'json' js.com.afr AppSpider (Rapid7) Use the VulnerabilitiesSummary.xml file found in the zipped report download.\nAuditJS (OSSIndex) AuditJS scanning tool using OSSIndex database and generated with --json or -j option (https://www.npmjs.com/package/auditjs).\nauditjs ossi --json \u003e auditjs_report.json AWS Security Hub The JSON output from AWS Security Hub exported with the aws securityhub get-findings (https://docs.aws.amazon.com/cli/latest/reference/securityhub/get-findings.html) command.\nAWS Scout2 Scanner (deprecated) JS file in scout2-report/inc-awsconfig/aws_config.js.\nWarning AWS Scout2 Scanner is deprecated and has been replaced with ScoutSuite (https://github.com/nccgroup/ScoutSuite) upstream. Please switch to the new parser for ScoutSuite.  Warning This parser is disactivated by default in releases \u003e= 2.3.1 and will be removed in release \u003e= 3.x.x.  AWS Prowler Scanner Prowler file can be imported as a CSV (-M csv) or JSON (-M json) file.\nAzure Security Center Recommendations Scan Azure Security Center recommendations can be exported from the user interface in CSV format.\nBandit JSON report format\nBlackduck API Import findings from the BlackDuck API - no file required.\nFollow these steps to setup API importing:\n Configure the BlackDuck API Authentication details by navigating to Configuration / Tool Configuration, selecting the Tool Type to ‚ÄúBlackDuck API‚Äù, and Authentication Type ‚ÄúAPI Key‚Äù. Paste your BlackDuck API token in the ‚ÄúAPI Key‚Äù field. In the Product settings select ‚ÄúAdd API Scan Configuration‚Äù and select the previously added BlackDuck API Tool Configuration. Provide the ID of the project from which to import findings in the field Service key 1. Provide the version of the project from which to import findings in the field Service key 2. After this is done, you can import the findings by selecting ‚ÄúBlackDuck API‚Äù as the scan type.  Blackduck Hub 2 options:\n Import the zip file as can be created by Blackduck export. The zip file must contain the security.csv and files.csv in order to produce findings that bear file locations information. Import a single security.csv file. Findings will not have any file location information.  Brakeman Scan Import Brakeman Scanner findings in JSON format.\nBugcrowd Import Bugcrowd results in CSV format.\nBugcrowd API Import Bugcrowd submissions directly from the API using the API token. Set your API key directly in the format username:password in the API Token input, it will be added to the header 'Authorization': 'Token {}'.format(self.api_token), For each product, you can configure 2 things:\n Service key 1: the bugcrowd program code (it‚Äôs the slug name in the url for the program, url safe) Service key 2: the bugcrowd target name (the full name, it will be url-encoded, you can find it in https://tracker.bugcrowd.com//settings/scope/target_groups)  It can be left empty so that all program submissions are imported    That way, per product, you can use the same program but separate by target, which is a fairly common way of filtering/grouping Bugcrowd. Adding support for a 3rd filtering would be possible with Service Key 3, feel free to make a PR.\nBundler-Audit Import the text output generated with bundle-audit check\nBurp XML When the Burp report is generated, the recommended option is Base64 encoding both the request and response fields - e.g. check the box that says \"Base64-encode requests and responses\". These fields will be processed and made available in the 'Finding View' page.\nBurp Enterprise Scan Import HTML reports from Burp Enterprise Edition\nBurp GraphQL Import the JSON data returned from the BurpSuite Enterprise GraphQL API. Append all the issues returned to a list and save it as the value for the key ‚ÄúIssues‚Äù. There is no need to filter duplicates, the parser will automatically combine issues with the same name.\nExample:\n{ \"Issues\": [ { \"issue_type\": { \"name\": \"Cross-site scripting (reflected)\", \"description_html\": \"Issue Description\", \"remediation_html\": \"Issue Remediation\", \"vulnerability_classifications_html\": \"\u003cli\u003e\u003ca href=\\\"https://cwe.mitre.org/data/definitions/79.html\\\"\u003eCWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')\u003c/a\u003e\u003c/li\u003e\", \"references_html\": \"\u003cli\u003e\u003ca href=\\\"https://portswigger.net/web-security/cross-site-scripting\\\"\u003eCross-site scripting\u003c/a\u003e\u003c/li\u003e\" }, \"description_html\": \"Details\", \"remediation_html\": \"Remediation Details\", \"severity\": \"high\", \"path\": \"/burp\", \"origin\": \"https://portswigger.net\", \"evidence\": [ { \"request_index\": 0, \"request_segments\": [ { \"data_html\": \"GET\" }, { \"highlight_html\": \"data\" }, { \"data_html\": \" HTTP More data\" } ] }, { \"response_index\": 0, \"response_segments\": [ { \"data_html\": \"HTTP/2 200 OK \" }, { \"highlight_html\": \"data\" }, { \"data_html\": \"More data\" } ] } ] } ] } Example GraphQL query to get issue details:\nquery Issue ($id: ID!, $serial_num: ID!) { issue(scan_id: $id, serial_number: $serial_num) { issue_type { name description_html remediation_html vulnerability_classifications_html references_html } description_html remediation_html severity path origin evidence { ... on Request { request_index request_segments { ... on DataSegment { data_html } ... on HighlightSegment { highlight_html } } } ... on Response { response_index response_segments { ... on DataSegment { data_html } ... on HighlightSegment { highlight_html } } } } } } CargoAudit Scan Import JSON output of cargo-audit scan report https://crates.io/crates/cargo-audit\nCheckov Report Import JSON reports of Infrastructure as Code vulnerabilities.\nClair Scan Import JSON reports of Docker image vulnerabilities.\nClair Klar Scan Import JSON reports of Docker image vulnerabilities from clair klar client.\nCobalt.io Scan CSV Report\nCobalt.io API Import Import findings from the Cobalt.io API - no file required.\nFollow these steps to setup API importing:\n Configure the Cobalt.io Authentication details by navigating to Configuration / Tool Configuration, selecting the Tool Type to ‚ÄúCobalt.io‚Äù, and Authentication Type ‚ÄúAPI Key‚Äù. Paste your Cobalt.io API key in the ‚ÄúAPI Key‚Äù field and the desired org token in the ‚ÄúExtras‚Äù field. In the Product settings select ‚ÄúAdd API Scan Configuration‚Äù and select the previously added Cobalt.io Tool Configuration. Provide the ID of the asset from which to import findings in the field Service key 1. The ID can be found at the end of the URL when viewing the asset in your browser. After this is done, you can import the findings by selecting ‚ÄúCobalt.io API Import‚Äù as the scan type. If you have more than one asset configured, you must also select which Cobalt.io API Scan Configuratio to use.  CodeQL CodeQL can be used to generate a SARIF report, that can be imported into Defect Dojo:\ncodeql database analyze db python-security-and-quality.qls --sarif-add-snippets --format=sarif-latest --output=security-extended.sarif The same can be achieved by running the CodeQL GitHub action with the add-snippet property set to true.\nCoverity API Export Coverity API view data in JSON format (/api/viewContents/issues endpoint).\nCurrently these columns are mandatory:\n displayType (Type in the UI) displayImpact (Impact in the UI) status (Status in the UI) firstDetected (First Detected in the UI)  Other supported attributes: cwe, displayFile, occurrenceCount and firstDetected\nCrashtest Security Import JSON Report Import XML Report in JUnit Format\nCredScan Report Import CSV credential scanner reports\nContrast Scanner CSV Report\nCheckmarx  Checkmarx Scan, Checkmarx Scan detailed: XML report from Checkmarx SAST (source code analysis) Checkmarx OSA: json report from Checkmarx Open Source Analysis (dependencies analysis)  To generate the OSA report using Checkmarx CLI: ./runCxConsole.sh OsaScan -v -CxServer \u003c...\u003e -CxToken \u003c..\u003e -projectName \u003c...\u003e -enableOsa -OsaLocationPath \u003clib_folder\u003e -OsaJson \u003coutput_folder\u003e\nThat will generate three files, two of which are needed for defectdojo. Build the file for defectdojo with the jq utility: jq -s . CxOSAVulnerabilities.json CxOSALibraries.json\nCloudsploit (AquaSecurity) From: https://github.com/aquasecurity/cloudsploit . Import the JSON output.\nCycloneDX CycloneDX is a lightweight software bill of materials (SBOM) standard designed for use in application security contexts and supply chain component analysis.\nFrom: https://www.cyclonedx.org/\nExample with Anchore Grype:\n./grype defectdojo/defectdojo-django:1.13.1 -o cyclonedx \u003e report.xml Example with cyclonedx-bom tool:\npip install cyclonedx-bom cyclonedx-py Usage: cyclonedx-py [OPTIONS] Options: -i \u003cpath\u003e - the alternate filename to a frozen requirements.txt -o \u003cpath\u003e - the bom file to create -j - generate JSON instead of XML DawnScanner Import report in JSON generated with -j option\nDependency Check OWASP Dependency Check output can be imported in Xml format. This parser ingests the vulnerable dependencies and inherits the suppressions.\n Suppressed vulnerabilities are tagged with the tag: suppressed. Suppressed vulnerabilities are marked as inactive, but not as mitigated. If the suppression is missing any \u003cnotes\u003e tag, it tags them as no_suppression_document. Related vulnerable dependencies are tagged with related tag.  Dependency Track Dependency Track has implemented a DefectDojo integration. Information about how to configure the integration is documented here: https://docs.dependencytrack.org/integrations/defectdojo/\nAlternatively, the Finding Packaging Format (FPF) from OWASP Dependency Track can be imported in JSON format. See here for more info on this JSON format: https://docs.dependencytrack.org/integrations/file-formats/\nDrHeader Import of JSON report from https://github.com/Santandersecurityresearch/DrHeader\nDockle Report Import JSON container image linter reports https://github.com/goodwithtech/dockle\ndocker-bench-security Scanner Import JSON reports of OWASP docker-bench-security. docker-bench-security is a script that make tests based on CIS Docker Benchmark.\nDetect-secrets Import of JSON report from https://github.com/Yelp/detect-secrets\nEdgescan Import Edgescan vulnerabilities by JSON file or API - no file required.\nFollow these steps to setup API importing:\n Configure the Edgescan authentication details by navigating to Configuration -\u003e Tool Configuration -\u003e Add Tool Configuration. Enter a Name, select the Tool Type ‚ÄúEdgescan‚Äù, Authentication Type ‚ÄúAPI Key‚Äù, paste your Edgescan API key in the API Key field, and click Submit. In the Product settings select Add API Scan Configuration and select the previously added Edgescan tool configuration. Provide the edgescan asset ID(s) that you wish to import the findings for in the field Service key 1. Multiple asset IDs should be comma separated with no spacing. After this is done, you can import the findings on the Product page through Findings -\u003e Import Scan Results. Select ‚ÄúEdgescan Scan‚Äù as the Scan type, the API scan configuration from the last step, and click Import.  ESLint ESLint Json report format (-f json)\nFortify Import Findings from XML file format.\nGeneric Findings Import Import Generic findings in CSV or JSON format.\nAttributes supported for CSV:\n Date: Date of the finding in mm/dd/yyyy format. Title: Title of the finding CweId: Cwe identifier, must be an integer value. Url: Url associated with the finding. Severity: Severity of the finding. Must be one of Info, Low, Medium, High, or Critical. Description: Description of the finding. Can be multiple lines if enclosed in double quotes. Mitigation: Possible Mitigations for the finding. Can be multiple lines if enclosed in double quotes. Impact: Detailed impact of the finding. Can be multiple lines if enclosed in double quotes. References: References associated with the finding. Can be multiple lines if enclosed in double quotes. Active: Indicator if the finding is active. Must be empty, TRUE or FALSE Verified: Indicator if the finding has been verified. Must be empty, TRUE, or FALSE FalsePositive: Indicator if the finding is a false positive. Must be TRUE, or FALSE. Duplicate:Indicator if the finding is a duplicate. Must be TRUE, or FALSE  The CSV expects a header row with the names of the attributes.\nExample of JSON format:\n{ \"findings\": [ { \"title\": \"test title with endpoints as dict\", \"description\": \"Some very long description with\\n\\n some UTF-8 chars √† qu'il est beau\", \"severity\": \"Medium\", \"mitigation\": \"Some mitigation\", \"date\": \"2021-01-06\", \"cve\": \"CVE-2020-36234\", \"cwe\": 261, \"cvssv3\": \"CVSS:3.1/AV:N/AC:L/PR:H/UI:R/S:C/C:L/I:L/A:N\", \"file_path\": \"src/first.cpp\", \"line\": 13, \"endpoints\": [ { \"host\": \"exemple.com\" } ] }, { \"title\": \"test title with endpoints as strings\", \"description\": \"Some very long description with\\n\\n some UTF-8 chars √† qu'il est beau2\", \"severity\": \"Critical\", \"mitigation\": \"Some mitigation\", \"date\": \"2021-01-06\", \"cve\": \"CVE-2020-36235\", \"cwe\": 287, \"cvssv3\": \"CVSS:3.1/AV:N/AC:L/PR:H/UI:R/S:C/C:L/I:L/A:N\", \"file_path\": \"src/two.cpp\", \"line\": 135, \"endpoints\": [ \"http://urlfiltering.paloaltonetworks.com/test-command-and-control\", \"https://urlfiltering.paloaltonetworks.com:2345/test-pest\" ] }, { \"title\": \"test title\", \"description\": \"Some very long description with\\n\\n some UTF-8 chars √† qu'il est beau2\", \"severity\": \"Critical\", \"mitigation\": \"Some mitigation\", \"date\": \"2021-01-06\", \"cve\": \"CVE-2020-36236\", \"cwe\": 287, \"cvssv3\": \"CVSS:3.1/AV:N/AC:L/PR:H/UI:R/S:C/C:L/I:L/A:N\", \"file_path\": \"src/threeeeeeeeee.cpp\", \"line\": 1353 } ] } This parser support an attributes that accept files as Base64 strings. These files are attached to the respective findings.\nExample:\n{ \"title\": \"My wonderful report\", \"findings\": [ { \"title\": \"Vuln with image\", \"description\": \"Some very long description\", \"severity\": \"Medium\", \"files\": [ { \"title\": \"Screenshot from 2017-04-10 16-54-19.png\", \"data\": \"iVBORw0KGgoAAAANSUhEUgAABWgAAAK0CAIAAAARSkPJAAAAA3N\u003c...\u003eTkSuQmCC\" } ] } ] } Ggshield Import Ggshield findings in JSON format.\nGosec Scanner Import Gosec Scanner findings in JSON format.\nGitleaks Import Gitleaks findings in JSON format.\nGitLab SAST Report Import SAST Report vulnerabilities in JSON format: https://docs.gitlab.com/ee/user/application_security/sast/#reports-json-format\nGitLab Dependency Scanning Report Import Dependency Scanning Report vulnerabilities in JSON format: https://docs.gitlab.com/ee/user/application_security/dependency_scanning/#reports-json-format\nGithub Vulnerability Import findings from Github vulnerability scan (GraphQL Query): https://help.github.com/en/github/managing-security-vulnerabilities\nCurrently the parser is able to manage only RepositoryVulnerabilityAlert object. The parser has some kind of search feature which detect the data in the report.\nHere is the mandatory objects and attributes:\nvulnerabilityAlerts (RepositoryVulnerabilityAlert object) + id + createdAt (optional) + vulnerableManifestPath (optional) + state (optional) + securityVulnerability (SecurityVulnerability object) + severity (CRITICAL/HIGH/LOW/MODERATE) + package (optional) + name (optional) + advisory (SecurityAdvisory object) + description + summary + description + identifiers + value + references (optional) + url (optional) + cvss (optional) + score (optional) + vectorString (optional) + cwes (optional) References:\n https://docs.github.com/en/graphql/reference/objects#repositoryvulnerabilityalert https://docs.github.com/en/graphql/reference/objects#securityvulnerability  Github v4 graphql query to fetch data:\nquery getVulnerabilitiesByOwner($owner: String!) { search(query: $owner, type: REPOSITORY, first: 100) { nodes { ... on Repository { name vulnerabilityAlerts(last: 100) { nodes { id securityVulnerability { severity package { name } advisory { description summary identifiers { type value } references { url } } } } } } } } } Another example of Python script that query one repository:\nimport json import requests query = \"\"\" query getVulnerabilitiesByRepoAndOwner($name: String!, $owner: String!) { repository(name: $name, owner: $owner) { vulnerabilityAlerts(first: 100) { nodes { id createdAt securityVulnerability { severity package { name ecosystem } advisory { description summary identifiers { value type } references { url } cvss { vectorString } } } vulnerableManifestPath } } } } \"\"\" token = '...' # generated from GitHub settings headers = {\"Authorization\": \"Bearer \" + token} request = requests.post(url='https://api.github.com/graphql', json={ \"operationName\": \"getVulnerabilitiesByRepoAndOwner\", 'query': query, 'variables': { 'name': 'gogoph', 'owner': 'damiencarol' } }, headers=headers) result = request.json() print(json.dumps(result, indent=2)) Hadolint Hadolint Dockerfile scan in json format.\nHarbor Vulnerability Import findings from Harbor registry container scan: https://github.com/goharbor/harbor\nHorusec Import findings from Horusec scan.\n./horusec_linux_x64 start -O=report.json -o json -i=\"tests/\" References:\n GitHub repository  HuskyCI Report Import JSON reports from HuskyCI\nHydra Import JSON reports from THC Hydra.\nHydra can discover weak login credentials on different types of services (e.g. RDP).\nAs Hydra cannot provide a severity rating (as it doesn‚Äôt know how severe a weak login is at this scanned service), all imported findings will be rated ‚ÄòHigh‚Äô.\nSample JSON report:\n{ \"errormessages\": [ \"[ERROR] Error Message of Something\", \"[ERROR] Another Message\", \"These are very free form\" ], \"generator\": { \"built\": \"2019-03-01 14:44:22\", \"commandline\": \"hydra -b jsonv1 -o results.json ... ...\", \"jsonoutputversion\": \"1.00\", \"server\": \"127.0.0.1\", \"service\": \"http-post-form\", \"software\": \"Hydra\", \"version\": \"v8.5\" }, \"quantityfound\": 1, \"results\": [ { \"host\": \"127.0.0.1\", \"login\": \"bill@example.com\", \"password\": \"bill\", \"port\": 9999, \"service\": \"http-post-form\" } ], \"success\": false } IBM AppScan DAST XML file from IBM App Scanner.\nImmuniweb Scan XML Scan Result File from Immuniweb Scan.\nIntSights Report IntSights Threat Command is a commercial Threat Intelligence platform that monitors both the open and dark web to identify threats for the Assets you care about (Domain Names, IP addresses, Brand Names, etc.).\nManual Import Use the Export CSV feature in the IntSights Threat Command GUI to create an IntSights Alerts.csv file. This CSV file can then be imported into Defect Dojo.\nAutomated Import The IntSights get-complete-alert API only returns details for a single alert. To automate the process, individually fetch details for each alert and append to a list. The list is then saved as the value for the key ‚ÄúAlerts‚Äù. This JSON object can then be imported into Defect Dojo.\nExample:\n{ \"Alerts\":[ { \"_id\":\"5c80egf83b4a3900078b6be6\", \"Details\":{ \"Source\":{ \"URL\":\"https://www.htbridge.com/websec/?id=ABCDEF\", \"Date\":\"2018-03-08T00:01:02.622Z\", \"Type\":\"Other\", \"NetworkType\":\"ClearWeb\" }, \"Images\":[ \"5c80egf833963a40007e01e8d\", \"5c80egf833b4a3900078b6bea\", \"5c80egf834626bd0007bd64db\" ], \"Title\":\"HTTP headers weakness in example.com web server\", \"Tags\":[], \"Type\":\"ExploitableData\", \"Severity\":\"Critical\", \"SubType\":\"VulnerabilityInTechnologyInUse\", \"Description\":\"X-XSS-PROTECTION and CONTENT-SECURITY-POLICY headers were not sent by the server, which makes it vulnerable for various attack vectors\" }, \"Assignees\":[ \"5c3c8f99903dfd0006ge5e61\" ], \"FoundDate\":\"2018-03-08T00:01:02.622Z\", \"Assets\":[ { \"Type\":\"Domains\", \"Value\":\"example.com\" } ], \"TakedownStatus\":\"NotSent\", \"IsFlagged\":false, \"UpdateDate\":\"2018-03-08T00:01:02.622Z\", \"RelatedIocs\":[], \"RelatedThreatIDs\":[], \"Closed\":{ \"IsClosed\":false } } ] }  JFrogXRay Import the JSON format for the \"Security Export\" file. Use this importer for Xray version 2.X\nJFrog XRay Unified Import the JSON format for the \"Security \u0026 Compliance | Reports\" export. Jfrog‚Äôs Xray tool is an add-on to their Artifactory repository that does Software Composition Analysis, see https://www.jfrog.com/confluence/display/JFROG/JFrog+Xray for more information. \"Xray Unified\" refers to Xray Version 3.0 and later.\nJFrog Xray API Summary Artifact Scan Import the JSON format from the Arifact Summary API call https://www.jfrog.com/confluence/display/JFROG/Xray+REST+API#XrayRESTAPI-ArtifactSummary\nKiuwan Scanner Import Kiuwan Scan in CSV format. Export as CSV Results on Kiuwan.\nkube-bench Scanner Import JSON reports of Kubernetes CIS benchmark scans.\nKICS Scanner Import of JSON report from https://github.com/Checkmarx/kics\nMeterian Scanner The Meterian JSON report output file can be imported.\nMicrofocus Webinspect Scanner Import XML report\nMobSF Scanner Export a JSON file using the API, api/v1/report_json.\nMobsfscan Import JSON report from https://github.com/MobSF/mobsfscan\nMozilla Observatory Scanner Import JSON report.\nNessus (Tenable) Reports can be imported in the CSV, and .nessus (XML) report formats.\nNessus WAS (Tenable) Reports can be imported in the CSV, and .nessus (XML) report formats.\nNetsparker Vulnerabilities List - JSON report\nNexpose XML 2.0 (Rapid7) Use the full XML export template from Nexpose.\nNikto Nikto web server scanner - https://cirt.net/Nikto2\nThe current parser support 3 sources:\n XML output (old) new XML output (with nxvmlversion=\"1.2\" type) JSON output  See: https://github.com/sullo/nikto\nNmap XML output (use -oX)\nNode Security Platform Node Security Platform (NSP) output file can be imported in JSON format.\nNPM Audit Node Package Manager (NPM) Audit plugin output file can be imported in JSON format. Only imports the 'advisories' subtree.\nNuclei Import JSON output of nuclei scan report https://github.com/projectdiscovery/nuclei\nOpenscap Vulnerability Scan Import Openscap Vulnerability Scan in XML formats.\nOpenVAS CSV Import OpenVAS Scan in CSV format. Export as CSV Results on OpenVAS.\nOssIndex Devaudit Import JSON formatted output from [OSSIndex Devaudit](https://github.com/sonatype-nexus-community/DevAudit).\nOss Review Toolkit Import ORT Evaluated model reporter in JSON Format. (Example)[https://github.com/DefectDojo/sample-scan-files/blob/master/ort/evaluated-model-reporter-output.json]\nPHP Security Audit v2 Import PHP Security Audit v2 Scan in JSON format.\nPHP Symfony Security Checker Import results from the PHP Symfony Security Checker.\nProbely Synchronize Probely Plus findings with DefectDojo.\nTo setup this integration set the DefectDojo URL and API key on the Integrations page on Probely. Then, select which Product, Engagement, and, optionally, the Test you want to synchronize to. The API key needs to belong to a user with write access to the product.\nWorks with DefectDojo 1.5.x and 1.6.x. Probely also supports non-public DefectDojo instances.\nFor detailed instructions on how to configure Probely and DefectDojo, see https://help.probely.com/en/articles/3811515-how-to-integrate-probely-with-defectdojo\nPWN Security Automation Framework  (Main Page)[https://github.com/0dayinc/pwn] pwn_sast: Import the JSON results generated by the pwn_sast Driver. This driver scans source code repositories for security anti-patterns that may result in vulnerability identification. More driver results coming soon‚Ä¶  Qualys Scan Qualys output files can be imported in API XML format. Qualys output files can be imported in WebGUI XML format.\nA CSV formatted Qualys Scan Report can also be used. Ensure the following values are checked in the Scan Report Template config:\nCVSS Version = CVSSv3\n Vulnerability Details  Threat Impact   Solution  Patches and Workarounds Virtual Patches and Mitigating Controls   Results  Qualys Webapp Scan Qualys WebScan output files can be imported in XML format.\nRetire.js Retire.js JavaScript scan (--js) output file can be imported in JSON format.\nRisk Recon API Importer Import findings from Risk Recon via the API. Configure your own JSON report as follows\n{ \"url_endpoint\": \"https://api.riskrecon.com/v1\", \"api_key\": \"you-api-key\", \"companies\": [ { \"name\": \"Company 1\", \"filters\": { \"domain_name\": [], \"ip_address\": [\"127.0.0.1\"], \"host_name\": [\"localhost\"], \"asset_value\": [], \"severity\": [\"critical\", \"high\"], \"priority\": [], \"hosting_provider\": [], \"country_name\": [] } }, { \"name\": \"Company 2\", \"filters\": { \"ip_address\": [\"0.0.0.0\"] } } ], \"filters\": { \"domain_name\": [], \"ip_address\": [], \"host_name\": [], \"asset_value\": [], \"severity\": [\"critical\"], \"priority\": [], \"hosting_provider\": [], \"country_name\": [] } }  More than one company finding list can be queried with it's own set of filters. Company 1 shows all available fitlers, while Company 2 shows that empty filters need not be present. To query all companies in your Risk Recon instance, simple remove the \"companies\" field entirely. If the \"companies\" field is not present, and filtering is still requested, the \"filters\" field can be used to filter all findings across all companies. It carries the same behavior as the company filters. The \"filters\" field is disregarded in the prescense of the \"companies\" field. Removing both fields will allow retrieval of all findings in the Risk Recon instance.  Rubocop Scan Import Rubocop JSON scan report (with option -f json).\nRusty Hog parser From: https://github.com/newrelic/rusty-hog Import the JSON output. Rusty Hog is a secret scanner built in Rust for performance, and based on TruffleHog which is written in Python.\nDefectDojo currently supports the parsing of the following Rusty Hog JSON outputs:\n Choctaw Hog: Scans for secrets in a Git repository. Duroc Hog: Scans for secrets in directories, files, and archives. Gottingen Hog: Scans for secrets in a JIRA issue. Essex Hog: Scans for secrets in a Confluence page.  SARIF OASIS Static Analysis Results Interchange Format (SARIF). SARIF is supported by many tools. More details about the format here: https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=sarif\nInformation SARIF parser customizes the Test_Type with data from the report. For example, a report with Dockle as a driver name will produce a Test with a Test_Type named Dockle Scan (SARIF)  Warning Current implementation is limited and will aggregate all the findings in the SARIF file in one single report.  Support for de-duplication (fingerprinting) SARIF parser take into account data for fingerprinting. It‚Äôs base on fingerprints and partialFingerprints properties. It‚Äôs possible to activate de-duplication based on this data by customizing settings.\n# in your settings.py file DEDUPLICATION_ALGORITHM_PER_PARSER[\"SARIF\"] = DEDUPE_ALGO_UNIQUE_ID_FROM_TOOL_OR_HASH_CODE Scantist Scan Scantist is an open source management platform. Scan and remediate open source security, licensing and compliance risks across your software development lifecycle. Here you can find more information: https://scantist.com/\nScoutSuite Multi-Cloud security auditing tool. It uses APIs exposed by cloud providers. Scan results are located at scan-reports/scoutsuite-results/scoutsuite\\_\\*.json files. Multiple scans will create multiple files if they are runing agains different Cloud projects. See https://github.com/nccgroup/ScoutSuite\nSemgrep JSON Report Import Semgrep output (‚Äìjson)\nSKF Scan Output of SKF Sprint summary export.\nSnyk Snyk output file (snyk test --json \u003e snyk.json) can be imported in JSON format. Only SCA (Software Composition Analysis) report is supported (SAST report not supported yet).\nSonarQube Scan (Aggregates findings per cwe, title, description, file_path.) SonarQube output file can be imported in HTML format.\nTo generate the report, see https://github.com/soprasteria/sonar-report\nVersion: \u003e= 1.1.0\nSonarQube Scan Detailed (Import all findings from SonarQube html report.) SonarQube output file can be imported in HTML format.\nTo generate the report, see https://github.com/soprasteria/sonar-report\nVersion: \u003e= 1.1.0\nSonarQube API Import SonarQube API will be accessed to gather the report. No report file is required.\nFollow these steps to setup the SonarQube API import:\n Configure the Sonarqube authentication details by navigating to Configuration / Tool Configuration. Note the url must be in the format of https://\u003csonarqube_host\u003e/api. Select the tool type to be SonarQube. By default the tool will import vulnerabilities issues and security hotspots only, but additional filters can be setup using the Extras field separated by commas (e.g. BUG,VULNERABILITY,CODE_SMELL). When using SonarCloud, you must also specify the Organization ID in the Extras field as follows OrgID=sonarcloud-organzation-ID. If also specifying issue type filters, please seperate the items in the Extras field by a vertical bar as follows BUG,VULNERABILITY,CODE_SMEL|OrgID=sonarcloud-organzation-ID In the Product settings add an API Scan Configuration. Service key 1 must be the SonarQube project key, which can be found by navigating to a specific project and selecting the value from the url https://\u003csonarqube_host\u003e/dashboard?id=key. When you do not provide a SonarQube project key, DefectDojo will use the name of the Product as the project key in SonarQube. If you would like to import findings from multiple projects, you can specify multiple keys as separated API Scan Configuration in the Product settings. If using SonarCloud, the orginization ID can be used from step 1, but it can be ovverirdden by supplying a different orginization ID in the Service key 2 input field. Once all of the settings are made, the SonarQube API Import will be able to import all vulnerability information from the SonarQube instance.  Multiple SonarQube API Configurations In the import or re-import dialog you can select which API Scan Configuration shall be used. If you do not choose any, DefectDojo will use the API Scan Configuration of the Product if there is only one defined or the SonarQube Tool Configuration if there is only one.\nMulti Branch Scanning If using a version of SonarQube with multi branch scanning, the branch tha be scanned can be supplied in the branch tag fieild at import/re-import time. If the branch does not exist, a notification will be generated in the alerts table indicating that branch to be imported does not exist. If a branch name is not supplied during import/re-import, the default branch of the SonarQube project will be used.\nNote:: If https is used for the SonarQube, the certificate must be trusted by the DefectDojo instance.\nSpotBugs XML report of textui cli.\nSonatype JSON output.\nSSL Labs JSON Output of ssllabs-scan cli.\nSslscan Import XML output of sslscan report.\nSslyze Scan XML report of SSLyze version 2 scan\nSSLyze 3 Scan (JSON) JSON report of SSLyze version 3 scan\nStackHawk HawkScan Import the JSON webhook event from StackHawk. For more information, check out our docs on hooking up StackHawk to Defect Dojo\nTestssl Scan Import CSV output of testssl scan report.\nTerrascan Import JSON output of terrascan scan report https://github.com/accurics/terrascan\nTrivy JSON report of trivy scanner.\nTrufflehog JSON Output of Trufflehog. Supports version 2 and 3 of https://github.com/trufflesecurity/trufflehog\nTrufflehog3 JSON Output of Trufflehog3, a fork of TruffleHog located at https://github.com/feeltheajf/truffleHog3\nTrustwave CSV output of Trustwave vulnerability scan.\nTwistlock JSON output of the twistcli tool. Example:\n./twistcli images scan \u003cREGISTRY/REPO:TAG\u003e --address https://\u003cSECURE_URL_OF_TWISTLOCK_CONSOLE\u003e --user \u003cUSER\u003e --details --output-file=\u003cPATH_TO_SAVE_JSON_FILE\u003e The CSV output from the UI is now also accepted.\nTFSec Import of JSON report from https://github.com/tfsec/tfsec\nVisual Code Grepper (VCG) VCG output can be imported in CSV or Xml formats.\nVeracode Detailed XML Report\nVeracode SourceClear Import Project CSV or JSON report\nVulners Import Vulners Audit results, no file required.\nFollow these steps to set up importing:\n Configure the Vulners API Key details by navigating to Configuration / Tool Configuration, selecting the Tool Type to ‚ÄúVulners‚Äù, and adding the API Key In the Product settings select ‚ÄúAdd API Scan Configuration‚Äù and select the previously added Vulners API Tool Configuration. After this is done, you can import the findings by selecting ‚ÄúVulners‚Äù as the scan type.  Detailed installation steps can be found in vulners documentation.\nUse following instructions to generate Vulners API Key.\nMore details about DefectDojo-plugin integration can be found at vulners integrations page.\nWapiti Scan Import XML report.\nWhitesource Scan Import JSON report\nWpscan Scanner Import JSON report.\nWfuzz JSON importer Import the result of Wfuzz (https://github.com/xmendez/wfuzz) if you export in JSON the result (wfuzz -o json -f myJSONReport.json,json).\nThe return code matching are directly put in Severity as follow(this is hardcoded in the parser actually).\n   HTTP Return Code Severity     200 High   401 Medium   403 Medium   407 Medium   500 Low    Whispers Import Whispers JSON results.\nhttps://github.com/adeptex/whispers\nXanitizer Import XML findings list report, preferably with parameter 'generateDetailsInFindingsListReport=true'.\nYarn Audit Import Yarn Audit scan report in JSON format. Use something like yarn audit --json \u003e yarn_report.json.\nZed Attack Proxy ZAP XML report format (with or without requests and responses).\n","categories":"","description":"DefectDojo has the ability to import scan reports from a large number of security tools.","excerpt":"DefectDojo has the ability to import scan reports from a large number ‚Ä¶","ref":"/django-DefectDojo/integrations/parsers/","tags":"","title":"Supported reports"},{"body":"DefectDojo's API is created using Django Rest Framework. The documentation of each endpoint is available within each DefectDojo installation at /api/v2/doc/ and can be accessed by choosing the API v2 Docs link on the user drop down menu in the header.\nThe documentation is generated using Django Rest Framework Yet Another Swagger Generator, and is interactive. On the top of API v2 docs is a link that generates an OpenAPI v2 spec.\nAs a preparation to move to OpenAPIv3, we have added an compatible spec and documentation at /api/v2/oa3/swagger-ui/\nTo interact with the documentation, a valid Authorization header value is needed. Visit the /api/v2/key/ view to generate your API Key (Token \u003capi_key\u003e) and copy the header value provided.\nEach section allows you to make calls to the API and view the Request URL, Response Body, Response Code and Response Headers.\nIf you‚Äôre logged in to the Defect Dojo web UI, you do not need to provide the authorization token.\nAuthentication The API uses header authentication with API key. The format of the header should be: :\nAuthorization: Token \u003capi.key\u003e  For example: :\nAuthorization: Token c8572a5adf107a693aa6c72584da31f4d1f1dcff  Alternative authentication method If you use an alternative authentication method for users, you may want to disable DefectDojo API tokens because it could bypass your authentication concept.\nUsing of DefectDojo API tokens can be disabled by specifying the environment variable DD_API_TOKENS_ENABLED to False.\nSample Code Here are some simple python examples and their results produced against the /users endpoint: :\nimport requests url = 'http://127.0.0.1:8000/api/v2/users' headers = {'content-type': 'application/json', 'Authorization': 'Token c8572a5adf107a693aa6c72584da31f4d1f1dcff'} r = requests.get(url, headers=headers, verify=True) # set verify to False if ssl cert is self-signed for key, value in r.__dict__.items(): print(f\"'{key}': '{value}'\") print('------------------') This code will return the list of all the users defined in DefectDojo. The json object result looks like : :\n[ { \"first_name\": \"Tyagi\", \"id\": 22, \"last_login\": \"2019-06-18T08:05:51.925743\", \"last_name\": \"Paz\", \"username\": \"dev7958\" }, { \"first_name\": \"saurabh\", \"id\": 31, \"last_login\": \"2019-06-06T11:44:32.533035\", \"last_name\": \"\", \"username\": \"saurabh.paz\" } ] Here is another example against the /users endpoint, this time we will filter the results to include only the users whose user name includes jay:\nimport requests url = 'http://127.0.0.1:8000/api/v2/users/?username__contains=jay' headers = {'content-type': 'application/json', 'Authorization': 'Token c8572a5adf107a693aa6c72584da31f4d1f1dcff'} r = requests.get(url, headers=headers, verify=True) # set verify to False if ssl cert is self-signed for key, value in r.__dict__.items(): print(f\"'{key}': '{value}'\") print('------------------') The json object result is: :\n[ { \"first_name\": \"Jay\", \"id\": 22, \"last_login\": \"2015-10-28T08:05:51.925743\", \"last_name\": \"Paz\", \"username\": \"jay7958\" }, { \"first_name\": \"\", \"id\": 31, \"last_login\": \"2015-10-13T11:44:32.533035\", \"last_name\": \"\", \"username\": \"jay.paz\" } ] See Django Rest Framework's documentation on interacting with an API for additional examples and tips.\nManually calling the API Tools like Postman can be used for testing the API.\nExample for importing a scan result:\n  Verb: POST\n  URI: http://localhost:8080/api/v2/import-scan/\n  Headers tab:\n add the authentication header  Key: Authorization Value: Token c8572a5adf107a693aa6c72584da31f4d1f1dcff      Body tab\n select \"form-data\", click \"bulk edit\". Example for a ZAP scan:    engagement:3 verified:true active:true lead:1 tags:test scan_type:ZAP Scan minimum_severity:Info skip_duplicates:true close_old_findings:false    Body tab\n Click \"Key-value\" edit Add a \"file\" parameter of type \"file\". This will trigger multi-part form data for sending the file content Browse for the file to upload    Click send\n  Clients / API Wrappers    Wrapper Status Notes     Specific python wrapper working (2021-01-21) API Wrapper including scripts for continous CI/CD uploading. Is lagging behind a bit on latest API features as we plan to revamp the API wrapper   Openapi python wrapper  proof of concept only where we found out the the OpenAPI spec is not perfect yet   Java library working (2021-08-30) Created by the kind people of SecureCodeBox   Image using the Java library working (2021-08-30)    .Net/C# library working (2021-06-08)    dd-import working (2021-08-24) dd-import is not directly an API wrapper. It offers some convenience functions to make it easier to import findings and language data from CI/CD pipelines.    Some of the api wrappers contain quite a bit of logic to ease scanning and importing in CI/CD environments. We are in the process of simplifying this by making the DefectDojo API smarter (so api wrappers / script can be dumber).\n","categories":"","description":"DefectDojo's API lets you automate tasks, e.g. uploading scan reports in CI/CD pipelines.","excerpt":"DefectDojo's API lets you automate tasks, e.g. uploading scan reports ‚Ä¶","ref":"/django-DefectDojo/integrations/api-v2-docs/","tags":"","title":"DefectDojo API v2"},{"body":"The documentation is build with Hugo and uses the theme Docsy. The source code of the documentation is located in the docs folder. Static files for the webside are build with github actions and are publish in the gh-pages branch.\nHow to run a local preview   Install Hugo. Make sure you have installed the extended version with Sass/SCSS support. Please note there are various Linux packages available on Hugo GitHub\n  Install JavaScript packages\nTo build or update your site‚Äôs CSS resources, you also need PostCSS to create the final assets. If you need to install it, you must have a recent version of NodeJS installed on your machine so you can use npm, the Node package manager. By default npm installs tools under the directory where you run npm install:\ncd docs npm install    Clone the DefectDojo git repository with the option --recurse-submodules. If you have already cloned the repository, make sure that you have checked out out the Docsy theme or use git submodule to check it out:\ncd docs/themes/docsy git submodule update --init --recursive    Switch to the docs folder and start the hugo server with hot reloading hugo server -D --config config.dev.toml\n  Visit http://localhost:1313/django-DefectDojo/dev.\n  See also the Docsy installation procedures for reference.\n","categories":"","description":"How to amend the documentation","excerpt":"How to amend the documentation","ref":"/django-DefectDojo/contributing/documentation/","tags":"","title":"Documentation"},{"body":"Risk Acceptance Findings cannot always be remediated or addressed for various reasons. A finding status can change to accepted by doing the following. Findings are accepted in the engagement view. To locate the engagement from the finding click the link to engagement as shown below.\nThen, in the engagement view click the plus icon in the 'Risk Acceptance' box and fill in the details to support the risk acceptance.\nThe engagement view is now updated with the risk.\nThe finding status changes to 'Accepted' with a link to the risk acceptance.\nDeduplication Deduplication is a feature that when enabled will compare findings to automatically identify duplicates. When deduplication is enabled, a list of deduplicated findings is added to the engagement view. The following image illustrates the option deduplication on engagement and deduplication on product level:\nUpon saving a finding, defectDojo will look at the other findings in the product or the engagement (depending on the configuration) to find duplicates\nWhen a duplicate is found:\n The newly imported finding takes status: inactive, duplicate An \"Original\" link is displayed after the finding status, leading to the original finding  There are two ways to use the deduplication:\n Deduplicate vulnerabilities in the same build/release. The vulnerabilities may be found by the same scanner (same scanner deduplication) or by different scanners (cross-scanner deduplication). this helps analysis and assessment of the technical debt, especially if using many different scanners; although detecting duplicates across scanners is not trivial as it requires a certain standardization. Track unique vulnerabilities across builds/releases so that defectDojo knows when it finds a vulnerability whether it has seen it before.  this allows you keep information attached to a given finding in a unique place: all further duplicate findings will point to the original one.\n  Deduplication configuration Global configuration The deduplication can be activated in \"System Settings\" by ticking \"Deduplicate findings\".\nAn option to delete duplicates can be found in the same menu, and the maximum number of duplicates to keep for the same finding can be configured.\nEngagement configuration When creating an engagement or later by editing the engagement, the \"Deduplication within engagement only\" checkbox can be ticked.\n If activated: Findings are only deduplicated within the same engagement. Findings present in different engagements cannot be duplicates Else: Findings are deduplicated across the whole product  Note that deduplication can never occur across different products.\nDeduplication algorithms The behavior of the deduplication can be configured for each parser in settings.dist.py (or settings.py after install) by configuring the DEDUPLICATION_ALGORITHM_PER_PARSER variable.\nThe available algorithms are:\n DEDUPE_ALGO_UNIQUE_ID_FROM_TOOL The deduplication occurs based on finding.unique_id_from_tool which is a unique technical id existing in the source tool. Few scanners populate this field currently. If you want to use this algorithm, you may need to update the scanner code beforehand.  Advantages:  If your source tool has a reliable means of tracking a unique vulnerability across scans, this configuration will allow defectDojo to use this ability.   Drawbacks:  Using this algorithm will not allow cross-scanner deduplication as other tools will have a different technical id. When the tool evolves, it may change the way the unique id is generated. In that case you won't be able to recognise that findings found in previous scans are actually the same as the new findings.     DEDUPE_ALGO_HASH_CODE The deduplication occurs based on finding.hash_code. The hash_code itself is configurable for each scanner in parameter HASHCODE_FIELDS_PER_SCANNER. DEDUPE_ALGO_UNIQUE_ID_FROM_TOOL_OR_HASH_CODE A finding is a duplicate with another if they have the same unique_id_from_tool OR the same hash_code.  Allows to use both  a technical deduplication (based on unique_id_from_tool) for a reliable same-parser deduplication and a functional one (based on hash_code configured on CWE+severity+file_path for example) for cross-parser deduplication     DEDUPE_ALGO_LEGACY This is algorithm that was in place before the configuration per parser was made possible, and also the default one for backward compatibility reasons.  Legacy algorithm basically deduplicates based on:  For static scanner: ['title', 'cwe', 'line', 'file_path', 'description'] For dynamic scanner: ['title', 'cwe', 'line', 'file_path', 'description', 'endpoints']    Note that there are some subtleties that may give unexpected results. Switch dojo.specific-loggers.deduplication to debug in settings.py to get more info in case of trouble.\n  Hash_code computation configuration The hash_code computation can be configured for each parser using the parameter HASHCODE_FIELDS_PER_SCANNER in settings.dist.py.\nThe parameter HASHCODE_ALLOWED_FIELDS list the fields from finding table that were tested and are known to be working when used as a hash_code. Don't hesitate to enrich this list when required (the code is generic and allows adding new fields by configuration only)\nNote that endpoints isn't a field from finding table but rather a meta value that will trigger a computation based on all the endpoints.\nWhen populating HASHCODE_FIELDS_PER_SCANNER, please respect the order of declaration of the fields: use the same order as in HASHCODE_ALLOWED_FIELDS: that will allow cross-scanner deduplication to function because the hash_code is computed as a sha-256 of concatenated values of the configured fields.\nTips:\n  It's advised to use fields that are standardized for a reliable deduplication, especially if aiming at cross-scanner deduplication. For example title and description tend to change when the tools evolve and don't allow cross-scanner deduplication\n Good candidates are  cwe or cve Adding the severity will make sure the deduplication won't be to aggressive (there are several families of XSS and sql injection for example, with various severities but the same cwe). Adding the file_path or endpoints is advised too.      The parameter HASHCODE_ALLOWS_NULL_CWE will allow switching to legacy algorithm when a null cwe is found for a given finding: this is to avoid getting many duplicates when the tool fails to give a cwe while we are expecting it.\n  Hashcode generation / regeneration When you change the hashcode configuration, it is needed to regenerated the hashcodes for all findings, or at least those findings found by scanners for which the configuration was updated.\nThis is sometimes also needed after an upgrade to a new Defect Dojo version, for example when we made changes to the hashcode configuration or calculation logic. We will mention this in the upgrade notes.\nTo regenerate the hashcodes, use the dedupe management command:\ndocker-compose exec uwsgi ./manage.py dedupe --hash_code_only This will only regenerated the hashcodes, but will not run any deduplication logic on existing findings. If you want to run deduplication again on existing findings to make sure any duplicates found by the new hashcode config are marked as such, run\ndocker-compose exec uwsgi ./manage.py dedupe The deduplication part of this command will run the deduplication for each finding in a celery task. If you want to run the deduplication in the foreground process, use:\ndocker-compose exec uwsgi ./manage.py dedupe --dedupe_sync Please note the deduplication process is resource intensive and can take a long time to complete (estimated ~7500 findings per minute when run in the foreground)\nDebugging deduplication There is a specific logger that can be activated in order to have details about the deduplication process : switch dojo.specific-loggers.deduplication to debug in settings.dist.py.\nDeduplication - APIv2 parameters  skip_duplicates: if true, duplicates are not inserted at all close_old_findings : if true, findings that are not duplicates and that were in the previous scan of the same type (example ZAP) for the same engagement (or product in case of \"close_old_findings_product_scope\") and that are not present in the new scan are closed (Inactive, Verified, Mitigated). close_old_findings_product_scope : if true, close_old_findings applies to all findings of the same type in the product. Note that \"Deduplication on engagement\" is no longer used to determine the scope of close_old_findings.  Deduplication / Similar findings Similar Findings Visualization:\n Similar Findings While viewing a finding, similar findings within the same product are listed along with buttons to mark one finding a duplicate of the other. Clicking the \"Use as original\" button on a similar finding will mark that finding as the original while marking the viewed finding as a duplicate. Clicking the \"Mark as duplicate\" button on a similar finding will mark that finding as a duplicate of the viewed finding. If a similar finding is already marked as a duplicate, then a \"Reset duplicate status\" button is shown instead which will remove the duplicate status on that finding along with marking it active again.  False Positive Removal DefectDojo allows users to tune out false positives by enabling False Positive History. This will track what engineers have labeled as false positive for a specific product and for a specific scanner. While enabled, when a tool reports the same issue that has been flagged as a false positive previously, it will automatically mark the finding as a false positive, helping to tune overly verbose security tools.\nFalse Positive Removal is not needed when using deduplication, and it is advised to not combine these two.\nService Level Agreement (SLA) DefectDojo allows you to maintain your security SLA and automatically remind teams whenever a SLA is about to get breached, or breaches.\nSimply indicate in the System Settings for each severity, how many days teams have to remediate a finding.\nSLA notification configuration There are 3 variables in the system settings that can be set for notifcations of SLA breaches. By default notifications are disabled. You can either choose to notify about breaches for findings that are only in ‚ÄòActive‚Äô or for any findings across the instance that are in Active, Verified. Furthermore, it is possible choose to only consider findings that have a JIRA issue linked to them.\nThere are 2 variables in the settings.py file that you can configure, to act on the global behavior.\nSLA_NOTIFY_PRE_BREACH = 3 SLA_NOTIFY_POST_BREACH = 7 The SLA_NOTIFY_PRE_BREACH is expressed in days. Whenever a finding's \"SLA countdown\" (time to remediate) drops to this number, a notification would be sent everyday, as scheduled by the crontab in settings.py, until the day it breaches.\nThe SLA_NOTIFY_POST_BREACH lets you define in days how long you want to be kept notified about findings that have breached the SLA. Passed that number, notifications will cease.\nWarning Be mindful of performance if you choose to have SLA notifications on non-verified findings, especially if you import a lot of findings through CI in 'active' state.  What notification channels for SLA notifications? The same as usual. You will notice that an extra SLA breach option is now present on the Notification page and also in the Product view.\nSLA notification with JIRA You can choose to also send SLA notification as JIRA comments, if your product is configured with JIRA. You can enable it at the JIRA configuration level or at the Product level.\nThe Product level JIRA notification configuration takes precendence over the global JIRA notification configuration.\nWhen is the SLA notification job run? The default setup will trigger the SLA notification code at 7:30am on a daily basis, as defined in the settings.py file. You can of course modify this schedule to your context.\n'compute-sla-age-and-notify': { 'task': 'dojo.tasks.async_sla_compute_and_notify', 'schedule': crontab(hour=7, minute=30), } Information The celery containers are the ones concerned with this configuration. If you suspect things are not working as expected, make sure they have the latest version of your settings.py file.  You can of course change this default by modifying that stanza.\nLaunching from the CLI You can also invoke the SLA¬†notification function from the CLI. For example, if run from docker-compose:\n$ docker-compose exec uwsgi /bin/bash -c 'python manage.py sla_notifications' Reports Instant reports Instant reports can be generated for:\n Product types Products Engagements Tests List of Findings Endpoints  Filtering is available on all report generation views to aid in focusing the report for the appropriate need.\nCustom reports Custom reports, generated with the Report Builder, allow you to select specific components to be added to the report. These include:\n Cover Page Table of Contents WYSIWYG Content Findings Vulnerable Endpoints Page Breaks  DefectDojo‚Äôs reports can be generated in HTML and AsciiDoc.\nMetrics DefectDojo provides a number of metrics visualization in order to help with reporting, awareness and to be able to quickly communicate a products/product type's security stance.\nThe following metric views are provided:\n Product Type Metrics This view provides graphs displaying Open Bug Count by Month, Accepted Bug Count by Month, Open Bug Count by Week, Accepted Bug Count by Week as well as tabular data on Top 10 Products by bug severity, Detail Breakdown of all reported findings, Opened Findings, Accepted Findings, Closed Findings, Trending Open Bug Count, Trending Accepted Bug Count, and Age of Issues.  Product Type Counts This view provides tabular data of Total Current Security Bug Count, Total Security Bugs Opened In Period, Total Security Bugs Closed In Period, Trending Total Bug Count By Month, Top 10 By Bug Severity, and Open Findings. This view works great for communication with stakeholders as it is a snapshot in time of the product.  Simple Metrics Provides tabular data for all Product Types. The data displayed in this view is the total number of S0, S1, S2, S3, S4, Opened This Month, and Closed This Month.  Engineer Metrics Provides graphs displaying information about a tester's activity.  Metrics Dashboard Provides a full screen, auto scroll view with many metrics in graph format. This view is great for large displays or \"Dashboards.\"   Users DefectDojo users inherit from django.contrib.auth.models.User.\nA username, first name, last name, and email address can be associated with each user. Additionally the following attributes describe the type of users:\n Active Designates whether this user should be treated as active and can login to DefectDojo. Unselect this instead of deleting accounts. Superuser status Designates that this user can configure the system and has all permissions for objects without explicitly assigning them.  A superuser may force a password reset for any user at any given time. This can be set when creating a new user, or when editing an existing one, requiring the user to change their password upon their next login.\nDefectDojo enforces the following password rules for all users:\n Must meet a length requirement of 9 characters Must be unique (not commonly used) Must contain one of each of the following: a number (0-9), uppercase letter (A-Z), lowercase letter (a-z), and symbol ()[]{}|~!@#$%^\u0026*_-+=;:`'\",\u003c\u003e./?  Calendar The calendar view provides a look at all the engagements and tests occurring during the month d, week or day displayed. Each entry is a direct link to the respective engagement or test view page.\nBenchmarks DefectDojo utilizes the OWASP ASVS Benchmarks to benchmark a product to ensure the product meets your application technical security controls. Benchmarks can be defined per the organizations policy for secure development and multiple benchmarks can be applied to a product.\nBenchmarks are available from the Product view. To view the configured benchmarks select the dropdown menu from the right hand drop down menu. You will find the selection near the bottom of the menu entitled: 'OWASP ASVS v.3.1'.\nIn the Benchmarks view for each product, the default level is ASVS Level\n On the top right hand side the drop down can be changed to the desired ASVS level (Level 1, Level 2 or Level 3). The publish checkbox will display the ASVS score on the product page and in the future this will be applied to reporting.  On the left hand side the ASVS score is displayed with the desired score, the % of benchmarks passed to achieve the score and the total enabled benchmarks for that AVSV level.\nAdditional benchmarks can be added/updated in the Django admin site. In a future release this will be brought out to the UI.\nEndpoint Meta Importer For heavy infrastructure scanning organizations, endpoints need to be as flexible as possible to get the most of DefectDojo. This flexibility comes in the form of Tags and custom fields. Tags allow users to filter, sort, and report objects in ways the base object is not totally proficient in doing.\nEndpoint Meta Importer provides a means to apply arbitrary tags and custom fields to endpoints in mass via a CSV file. Tags and customs fields are stored in the format of column:row.\nHere is a very simple example with only two columns:\nhostname | team | public_facing ------------------------------------------------------------------ sheets.google.com | data analytics | yes docs.google.com | language processing | yes feedback.internal.google.com | human resources | no The three endpoints hosts will be used to find existing endpoints with matching hosts, or create new endpoints, and then apply meta as follows:\nsheets.google.com (endpoint) -\u003e [ team:data analytics, public_facing:yes ] (tags) docs.google.com (endpoint) -\u003e [ team:language processing, public_facing:yes ] (tags) feedback.internal.google.com (endpoint) -\u003e [ team:human resources, public_facing:no ] (tags) Endpoint Meta Importer can be found in the Endpoint tab when viewing a Product\nNote: The field ‚Äúhostname‚Äù is required as it is used to query/create endpoints.\n","categories":"","description":"Various features help manage the findings.","excerpt":"Various features help manage the findings.","ref":"/django-DefectDojo/usage/features/","tags":"","title":"Features"},{"body":"Recommended Options  Docker Compose See instructions in DOCKER.md\nSaaS (Includes Support \u0026 Supports the Project) SaaS link\nAWS AMI (Supports the Project) Marketplace link, and complete walkthrough\n Options for the Brave  Kubernetes See instructions in KUBERNETES.md\nLocal install with godojo See instructions in README.md in the godojo repository\n Customizing of settings See Configuration\n","categories":"","description":"DefectDojo supports various installation options.","excerpt":"DefectDojo supports various installation options.","ref":"/django-DefectDojo/getting_started/installation/","tags":"","title":"Installation"},{"body":"","categories":"","description":"How to use DefectDojo to manage vulnerabilities","excerpt":"How to use DefectDojo to manage vulnerabilities","ref":"/django-DefectDojo/usage/","tags":"","title":"Usage"},{"body":"To manage expectations, we call this the wishlist. These are items we want to do, are discussing or pondering our minds:\n New modern UI / SPA New dashboarding / statistics New search engine Adopt a plugin framework to allow plugins for issue trackers, parsers, reports, etc More flexible model  ","categories":"","description":"Be careful what you wish for","excerpt":"Be careful what you wish for","ref":"/django-DefectDojo/contributing/wishlist/","tags":"","title":"Wishlist"},{"body":"Auth0 In the same way as with other identity providers, it‚Äôs now possible to leverage Auth0 to authenticate users on DefectDojo.\n  Inside your Auth0 dashboard create a new application (Applications / Create Application / Single Page Web Application).\n  On the new application set the following fields:\n Name: ‚ÄúDefectdojo‚Äù Allowed Callback URLs: https://the_hostname_you_have_dojo_deployed:your_server_port/complete/auth0/    Copy the following info from the application:\n Domain Client ID Client Secret    Now, edit the settings (see Configuration) with the following information:\nDD_SOCIAL_AUTH_AUTH0_OAUTH2_ENABLED=True DD_SOCIAL_AUTH_AUTH0_KEY=(str, '**YOUR_CLIENT_ID_FROM_STEP_ABOVE**'), DD_SOCIAL_AUTH_AUTH0_SECRET=(str,'**YOUR_CLIENT_SECRET_FROM_STEP_ABOVE**'), DD_SOCIAL_AUTH_AUTH0_DOMAIN=(str, '**YOUR_AUTH0_DOMAIN_FROM_STEP_ABOVE**'),    Restart DefectDojo, and you should now see a Login with Auth0 button on the login page.\n  Google New to DefectDojo, a Google account can now be used for Authentication, Authorization, and a DefectDojo user. Upon login with a Google account, a new user will be created if one does not already exist. The criteria for determining whether a user exists is based on the users username. In the event a new user is created, the username is that of the Google address without the domain. Once created, the user creation process will not happen again as the user is recalled by its username, and logged in. In order to make the magic happen, a Google authentication server needs to be created. Closely follow the steps below to guarantee success.\n  Navigate to the following address and either create a new account, or login with an existing one: Google Developers Console\n  Once logged in, find the key shaped button labeled Credentials on the left side of the screen. Click Create Credentials, and choose OAuth Client ID:\n  Select Web Applications, and provide a descriptive name for the client.\n  Add the pictured URLs in the Authorized Redirect URLs section. This part is very important. If there are any mistakes here, the authentication client will not authorize the request, and deny access.\n  Once all URLs are added, finish by clicking Create\n  Now with the authentication client created, the Client ID and Client Secret Key need to be copied over to the settings. Click the newly created client and copy the values:\n  Edit the settings (see Configuration) with the following information:\nDD_SOCIAL_AUTH_GOOGLE_OAUTH2_ENABLED=True, DD_SOCIAL_AUTH_GOOGLE_OAUTH2_KEY=(str, '**YOUR_CLIENT_ID_FROM_STEP_ABOVE**'), DD_SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET=(str, '**YOUR_CLIENT_SECRET_FROM_STEP_ABOVE**'),  To authorize users you will need to set the following:\nDD_SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS = ['example.com', 'example.org']  or\nDD_SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_EMAILS = ['\u003cemail@example.com\u003e']    OKTA In a similar fashion to that of Google, using OKTA as a OAuth2 provider carries the same attributes and a similar procedure. Follow along below.\n  Navigate to the following address and either create a new account, or login with an existing one: OKTA Account Creation\n  Once logged in, enter the Applications and click Add Application:\n  Select Web Applications.\n  Add the pictured URLs in the Login Redirect URLs section. This part is very important. If there are any mistakes here, the authentication client will not authorize the request, and deny access. Check the Implicit box as well.\n  Once all URLs are added, finish by clicking Done.\n  Return to the Dashboard to find the Org-URL. Note this value as it will be important in the settings file.\n  Now, with the authentication client created, the Client ID and Client Secret Key need to be copied over to the settings. Click the newly created client and copy the values:\n  Edit the settings (see Configuration) with the following information:\nDD_SOCIAL_AUTH_OKTA_OAUTH2_ENABLED=True, DD_SOCIAL_AUTH_OKTA_OAUTH2_KEY=(str, '**YOUR_CLIENT_ID_FROM_STEP_ABOVE**'), DD_SOCIAL_AUTH_OKTA_OAUTH2_SECRET=(str, '**YOUR_CLIENT_SECRET_FROM_STEP_ABOVE**'), DD_SOCIAL_AUTH_OKTA_OAUTH2_API_URL=(str, 'https://{your-org-url}/oauth2'),    If during the login process you get the following error: The ‚Äòredirect_uri‚Äô parameter must be an absolute URI that is whitelisted in the client app settings. and the redirect_uri HTTP GET parameter starts with http:// instead of https:// you need to add SOCIAL_AUTH_REDIRECT_IS_HTTPS = True in the settings.\nAzure Active Directory Azure AD Configuration You can now use your corporate Azure Active Directory to authenticate users to Defect Dojo. Users will be using your corporate Azure AD account (A.K.A. Office 365 identity) to authenticate via OAuth, and all the conditional access rules and benefits from Azure Active Directory will also apply to the Defect Dojo Authentication. Once the user signs in, it will try to match the UPN of the user to an existing e-mail from a user in Defect Dojo, and if no match is found, a new user will be created in Defect Dojo, associated with the unique id/value of the user provided by your Azure AD tenant. Then, you can assign roles to this user, such as ‚Äòsuperuser‚Äô.\n  Navigate to the following address and follow instructions to create a new app registration\n https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app    Once you register an app, take note of the following information:\n Application (client) ID Directory (tenant) ID Under Certificates \u0026 Secrets, create a new Client Secret    Under Authentication \u003e Redirect URIs, add a WEB type of uri where the redirect points to\n http://localhost:8080/complete/azuread-tenant-oauth2/ OR https://the_hostname_you_have_dojo_deployed:your_server_port/complete/azuread-tenant-oauth2/    Edit the settings (see Configuration) with the following information:\nDD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_KEY=(str, 'YOUR_APPLICATION_ID_FROM_STEP_ABOVE'), DD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_SECRET=(str, 'YOUR_CLIENT_SECRET_FROM_STEP_ABOVE'), DD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_TENANT_ID=(str, 'YOUR_DIRECTORY_ID_FROM_STEP_ABOVE'), DD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_ENABLED = True    Restart your Dojo, and you should now see a Login with Azure AD button on the login page which should magically work\n  Automatic Import of User-Groups To import groups from Azure AD users, the following environment variable needs to be set:\nDD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_GET_GROUPS=True   This will ensure the user is added to all the groups found in the Azure AD Token. Any missing groups will be created in DefectDojo (unless filtered). This group synchronization allows for product access via groups to limit the products a user can interact with. Do not activate Emit groups as role claims within the Azure AD ‚ÄúToken configuration‚Äù.\nTo prevent authorization creep, old Azure AD groups a user is not having anymore can be deleted with the following environment parameter:\nDD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_CLEANUP_GROUPS=True   To limit the amount of groups imported from Azure AD, a regular expression can be used as the following:\nDD_SOCIAL_AUTH_AZUREAD_TENANT_OAUTH2_GROUPS_FILTER='^team-.*' # or 'teamA|teamB|groupC'   Gitlab In a similar fashion to that of Google and OKTA, using Gitlab as a OAuth2 provider carries the same attributes and a similar procedure. Follow along below.\n  Navigate to your Gitlab settings page and got to the Applications section\n https://gitlab.com/profile/applications OR https://the_hostname_you_have_gitlab_deployed:your_gitlab_port/profile/applications    Choose a name for your application\n  For the Redirect URI, enter the DefectDojo URL with the following format\n https://the_hostname_you_have_dojo_deployed:your_server_port/complete/gitlab/    Edit the settings (see Configuration) with the following information:\nDD_SOCIAL_AUTH_GITLAB_KEY=(str, 'YOUR_APPLICATION_ID_FROM_STEP_ABOVE'), DD_SOCIAL_AUTH_GITLAB_SECRET=(str, 'YOUR_SECRET_FROM_STEP_ABOVE'), DD_SOCIAL_AUTH_GITLAB_API_URL=(str, 'https://gitlab.com'), DD_SOCIAL_AUTH_GITLAB_OAUTH2_ENABLED = True  Additionally, if you want to import your Gitlab projects as DefectDojo products, add the following line to your settings:\nDD_SOCIAL_AUTH_GITLAB_PROJECT_AUTO_IMPORT = True    Restart DefectDojo, and you should now see a Login with Gitlab button on the login page.\n  Keycloak There is also an option to use Keycloak as OAuth2 provider in order to authenticate users to Defect Dojo, also by using the social-auth plugin.\nHere are suggestion on how to configure Keycloak and DefectDojo:\nConfigure Keycloak (assuming you already have an existing realm, otherwise create one)\n Navigate to your keycloak realm and add a new client of type openid-connect. Choose a name for the client id and use this value below for DD_SOCIAL_AUTH_KEYCLOAK_KEY). In the client settings:  Set access type to confidential Under valid Redirect URIs, add the URI to your defect dojo installation, e.g. ‚Äòhttps://\u003cYOUR_DD_HOST\u003e/*‚Äô Under web origins, add the same (or ‚Äò+') Under Fine grained openID connect configuration -\u003e user info signed response algorithm: set to RS256 Under Fine grained openID connect configuration -\u003e request object signature algorithm: set to RS256 -\u003e save these settings in keycloak (hit save button)   Under Scope -\u003e Full Scope Allowed set to off Under mappers -\u003e add a custom mapper here:  Name: aud Mapper type: audience Included audience: select your client/client-id here Add ID to token: off Add access to token: on   Under credentials: copy the secret (and use as DD_SOCIAL_AUTH_KEYCLOAK_SECRET below) In your realm settings -\u003e keys: copy the ‚ÄúPublic key‚Äù (signing key) (use for DD_SOCIAL_AUTH_KEYCLOAK_PUBLIC_KEY below) In your realm settings -\u003e general -\u003e endpoints: look into openId endpoint configuration and look up your authorization and token endpoint (use them below)  Configure Defect Dojo Edit the settings (see Configuration) with the following information:\nDD_SESSION_COOKIE_SECURE=True, DD_CSRF_COOKIE_SECURE=True, DD_SECURE_SSL_REDIRECT=True, DD_SOCIAL_AUTH_KEYCLOAK_OAUTH2_ENABLED=True, DD_SOCIAL_AUTH_KEYCLOAK_PUBLIC_KEY=(str, '\u003cyour realm public key\u003e'), DD_SOCIAL_AUTH_KEYCLOAK_KEY=(str, '\u003cyour client id\u003e'), DD_SOCIAL_AUTH_KEYCLOAK_SECRET=(str, '\u003cyour keycloak client credentials secret\u003e'), DD_SOCIAL_AUTH_KEYCLOAK_AUTHORIZATION_URL=(str, '\u003cyour authorization endpoint\u003e'), DD_SOCIAL_AUTH_KEYCLOAK_ACCESS_TOKEN_URL=(str, '\u003cyour token endpoint\u003e')  or, alternatively, for helm configuration, add this to the extraConfig section:\nDD_SESSION_COOKIE_SECURE: 'True' DD_CSRF_COOKIE_SECURE: 'True' DD_SECURE_SSL_REDIRECT: 'True' DD_SOCIAL_AUTH_KEYCLOAK_OAUTH2_ENABLED: 'True' DD_SOCIAL_AUTH_KEYCLOAK_PUBLIC_KEY: '\u003cyour realm public key\u003e' DD_SOCIAL_AUTH_KEYCLOAK_KEY: '\u003cyour client id\u003e' DD_SOCIAL_AUTH_KEYCLOAK_SECRET: '\u003cyour keycloak client credentials secret\u003e' DD_SOCIAL_AUTH_KEYCLOAK_AUTHORIZATION_URL: '\u003cyour authorization endpoint\u003e' DD_SOCIAL_AUTH_KEYCLOAK_ACCESS_TOKEN_URL: '\u003cyour token endpoint\u003e' Optionally, you can set DD_SOCIAL_AUTH_KEYCLOAK_LOGIN_BUTTON_TEXT in order to customize the login button‚Äôs text caption.\nGitHub  Navigate to GitHub.com and follow instructions to create a new OAuth App https://docs.github.com/en/developers/apps/building-oauth-apps/creating-an-oauth-app Choose a name for your application For the Redirect URI, enter the DefectDojo URL with the following format  https://the_hostname_you_have_dojo_deployed:your_server_port/complete/github/   Edit the settings (see Configuration) with the following information: DD_SOCIAL_AUTH_GITHUB_KEY=(str, 'GitHub OAuth App Client ID'), DD_SOCIAL_AUTH_GITHUB_SECRET=(str, 'GitHub OAuth App Client Secret'), DD_SOCIAL_AUTH_GITHUB_OAUTH2_ENABLED = True  Restart DefectDojo, and you should now see a Login with GitHub button on the login page.  GitHub Enterprise  Navigate to your GitHub Enterprise Server and follow instructions to create a new OAuth App https://docs.github.com/en/enterprise-server/developers/apps/building-oauth-apps/creating-an-oauth-app Choose a name for your application For the Redirect URI, enter the DefectDojo URL with the following format  https://the_hostname_you_have_dojo_deployed:your_server_port/complete/github-enterprise/   Edit the settings (see Configuration) with the following information: DD_SOCIAL_AUTH_GITHUB_ENTERPRISE_KEY=(str, 'GitHub Enterprise OAuth App Client ID'), DD_SOCIAL_AUTH_GITHUB_ENTERPRISE_SECRET=(str, 'GitHub Enterprise OAuth App Client Secret'), DD_SOCIAL_AUTH_GITHUB_ENTERPRISE_URL=(str, 'https://github.\u003cyour_company\u003e.com/'), DD_SOCIAL_AUTH_GITHUB_ENTERPRISE_API_URL=(str, 'https://github.\u003cyour_company\u003e.com/api/v3/'), DD_SOCIAL_AUTH_GITHUB_ENTERPRISE_OAUTH2_ENABLED = True,  Restart DefectDojo, and you should now see a Login with GitHub Enterprise button on the login page.  SAML 2.0 In a similar direction to OAuth, this SAML addition provides a more secure perogative to SSO. For definitions of terms used and more information, see the plugin plugin homepage.\n  Navigate to your SAML IdP and find your metadata\n  Edit the settings (see Configuration) with the following information:\nDD_SAML2_ENABLED=(bool, **True**), # SAML Login Button Text DD_SAML2_LOGIN_BUTTON_TEXT=(str, 'Login with SAML'), # If the metadata can be accessed from a url, try the DD_SAML2_METADATA_AUTO_CONF_URL=(str, '\u003chttps://your_IdP.com/metadata.xml\u003e'), # Otherwise, downlaod a copy of the metadata into an xml file, and # list the path in DD_SAML2_METADATA_LOCAL_FILE_PATH DD_SAML2_METADATA_LOCAL_FILE_PATH=(str, '/path/to/your/metadata.xml'), # Fill in DD_SAML2_ATTRIBUTES_MAP to corresponding SAML2 userprofile attributes provided by your IdP DD_SAML2_ATTRIBUTES_MAP=(dict, { # format: SAML attrib:django_user_model 'Email': 'email', 'UserName': 'username', 'Firstname': 'first_name', 'Lastname': 'last_name' }), # May configure the optional fields    NOTE: DD_SAML2_ATTRIBUTES_MAP in k8s can be referenced as extraConfig (e.g. DD_SAML2_ATTRIBUTES_MAP: 'Email'='email', 'Username'='username'...)\nNOTE: DD_SITE_URL might also need to be set depending on the choices you make with the metadata.xml provider. (File versus URL).\n Checkout the SAML section in dojo/dojo/settings/settings.dist.py and verfiy if it fits your requirement. If you need help, take a look at the plugin documentation.\n  Restart DefectDojo, and you should now see a Login with SAML button (default setting of DD_SAML2_LOGIN_BUTTON_TEXT) on the login page.\n  NOTE: In the case when IDP is configured to use self signed (private) certificate, than CA needs to be specified by define environments variable REQUESTS_CA_BUNDLE that points to the path of private CA certificate.\nAdvanced Configuration The https://github.com/IdentityPython/djangosaml2 plugin has a lot of options. For details take a look at the plugin documentation. All default options in DefectDojo can overwritten in the local_settings.py. If you want to change the organization name, you can add the following lines:\nif SAML2_ENABLED: SAML_CONFIG['contact_person'] = [{ 'given_name': 'Extra', 'sur_name': 'Example', 'company': 'DefectDojo', 'email_address': 'dummy@defectdojo.com', 'contact_type': 'technical' }] SAML_CONFIG['organization'] = { 'name': [('DefectDojo', 'en')], 'display_name': [('DefectDojo', 'en')], }, Migration from django-saml2-auth Up to relase 1.15.0 the SAML integration was based on https://github.com/fangli/django-saml2-auth. Which the switch to djangosaml2 some parameters has changed:\n DD_SAML2_ASSERTION_URL: not necessary any more - automatically generated DD_SAML2_DEFAULT_NEXT_URL: not necessary any more - default forwarding from defectdojo is used DD_SAML2_NEW_USER_PROFILE: not possible any more - default profile is used, see User Permissions DD_SAML2_ATTRIBUTES_MAP: Syntax has changed DD_SAML2_CREATE_USER: Default value changed to False, to avoid security breaches  RemoteUser This implementation is suitable if the DefectDojo instance is placed behind HTTP Authentication Proxy. Dojo expects that the proxy will perform authentication and pass HTTP requests to the Dojo instance with filled HTTP headers. The proxy should check if an attacker is not trying to add a malicious HTTP header and bypass authentication.\nValues which need to be set:\n DD_AUTH_REMOTEUSER_ENABLED - Needs to be set to True DD_AUTH_REMOTEUSER_USERNAME_HEADER - Name of the header which contains the username DD_AUTH_REMOTEUSER_EMAIL_HEADER(optional) - Name of the header which contains the email DD_AUTH_REMOTEUSER_FIRSTNAME_HEADER(optional) - Name of the header which contains the first name DD_AUTH_REMOTEUSER_LASTNAME_HEADER(optional) - Name of the header which contains the last name DD_AUTH_REMOTEUSER_GROUPS_HEADER(optional) - Name of the header which contains the comma-separated list of groups; user will be assigned to these groups (missing groups will be created) DD_AUTH_REMOTEUSER_GROUPS_CLEANUP(optional) - Same as [#automatic-import-of-user-groups](AzureAD implementation) DD_AUTH_REMOTEUSER_TRUSTED_PROXY - Comma separated list of proxies; Simple IP and CIDR formats are supported DD_AUTH_REMOTEUSER_LOGIN_ONLY(optional) - Check Django documentation  WARNING: There is possible spoofing of headers (for all DD_AUTH_REMOTEUSER_xxx_HEADER values). Read Warning in Django documentation\nUser Permissions When a new user is created via the social-auth, only the default permissions are active. This means that the newly created user does not have access to add, edit, nor delete anything within DefectDojo. There are two parameters in the System Settings to influence the permissions for newly created users:\nDefault group When both the parameters Default group and Default group role are set, the new user will be a member of the given group with the given role, which will give him the respective permissions.\nLogin speed-up You can bypass the login form if you are only using SSO/Social authentication for login in by enabling these two environment variables:\nDD_SOCIAL_LOGIN_AUTO_REDIRECT: \"true\" DD_SOCIAL_AUTH_SHOW_LOGIN_FORM: \"false\" Login form fallback If you are using ‚Äúlogin speed-up‚Äù, it can be useful to be able to login by the standard way, for example when an admin user needs to log in because of a change of some settings or permissions. This feature is accessible by a visiting the URL \u003cDD_HOST\u003e/login?force_login_form.\nOther Providers In an effort to accommodate as much generality as possible, it was decided to implement OAuth2 with the social-auth ecosystem as it has a library of compatible providers with documentation of implementation. Conveniently, each provider has an identical procedure of managing the authenticated responses and authorizing access within a given application. The only difficulty is creating a new authentication client with a given OAuth2 provider.\n","categories":"","description":"OAuth2/SAML2 let users authenticate against enterprise directories.","excerpt":"OAuth2/SAML2 let users authenticate against enterprise directories.","ref":"/django-DefectDojo/integrations/social-authentication/","tags":"","title":"Authentication via OAuth2/SAML2"},{"body":"Regular releases The DefectDojo team aims to meet the following cadence:\n Minor releases at least once a month on the first Monday of the month Patch/Bugfix releases every Monday Security releases can come at any time.  In doubt, GitHub Actions are the source of truth. The releases are semi-automated right now, with a DefectDojo maintainer proceeding with each major step in the release. The steps for a regular release are:\n Create the release branch from dev or bugfix and prepare a PR against master (details) ‚Äì\u003e A maintainer verifies and manually merges the PR Tag, issue draft release and docker build+push (details) ‚Äì\u003e A maintainer massages the release-drafter notes and publishes the release A PR to merge master back to dev and bugfix is created to re-align the branches (details)  Security releases PRs that relate to security issues are done through security advisories which provide a way to work privately on code without prematurely disclosing vulnerabilities.\nRelease and hotfix model Diagrams created with plantUML. Find a web-based editor for PlantUML at https://www.planttext.com.\nDocumentation A dev version of the documentation built from the dev branch is available at DefectDojo Documentation - dev branch.\n``` @startuml participant ‚ÄúDev Branch‚Äù as dev #LightBlue participant ‚ÄúBugFix Branch‚Äù as bugfix #LightGreen participant ‚ÄúRelease Branch‚Äù as release #LightGoldenRodYellow participant ‚ÄúMaster Branch‚Äù as master #LightSalmon\n== Minor Release (Monthly) ==\ndev -\u003e release: Create branch ‚Äúrelease/2.x.0‚Äù release -\u003e master: Merge note right: Official Release\\n - Tag 2.x.0\\n - Push 2.x.0 to DockerHub master ‚Äì\u003e bugfix: Merge master into bugfix to realign master ‚Äì\u003e dev: Merge master back into dev\n== Patch/BugFix Release (Weekly) ==\nbugfix -\u003e release: Create branch ‚Äúrelease/2.x.y‚Äù release -\u003e master: Merge note right: Official Release\\n - Tag 2.x.y\\n - Push 2.x.y to DockerHub master -\u003e bugfix: Merge master back into bugfix to realign master ‚Äì\u003e dev: Merge master into dev to realign\n== Security Release (As Needed) ==\nmaster -\u003e release: Create branch ‚Äúrelease/2.x.y‚Äù release -\u003e master: Merge note right: Official Release\\n - Tag 2.x.y\\n - Push 2.x.y to DockerHub master ‚Äì\u003e bugfix: Merge master into bugfix to realign master ‚Äì\u003e dev: Merge master into dev to realign\n@enduml\n\u003c/div\u003e","categories":"","description":"How to create releases","excerpt":"How to create releases","ref":"/django-DefectDojo/contributing/branching-model/","tags":"","title":"Branching model"},{"body":"dojo/settings/settings.dist.py The main settings are all stored in dojo/settings/settings.dist.py. It is great to use this file as a reference what can be configured, but it shouldn‚Äôt be edited directly, because changes would be overridden when updating DefectDojo. There are several methods to change the default settings:\nEnvironment variables Most of these parameters can be set by environment variables.\nWhen you deploy DefectDojo via Docker Compose, you can set environment variables in docker-compose.yml. Be aware you have to set the variables for three services: uwsgi, celerybeat and celeryworker.\nWhen you deploy DefectDojo in a Kubernetes cluster, you can set environment variables as extraConfigs and extraSecrets in helm/defectdojo/values.yaml.\nEnvironment file (not with Docker Compose or Kubernetes) settings.dist.py reads environment variables from a file whose name is specified in the environment variable DD_ENV_PATH. If this variable is not set, the default .env.prod is used. The file must be located in the dojo/settings directory.\nAn example can be found in template_env.\nlocal_settings.py (not with Kubernetes) local_settings.py can contain more complex customizations such as adding MIDDLEWARE or INSTALLED_APP entries. This file is processed after settings.dist.py is processed, so you can modify settings delivered by Defect Dojo out of the box. The file must be located in the dojo/settings directory. Environment variables in this file must have no DD_ prefix. If the file is missing feel free to create it. Do not edit settings.dist.py directly.\nAn example can be found in dojo/settings/template-local_settings.\nIn Docker Compose release mode, files in docker/extra_settings/ (relative to the file docker-compose.yml) will be copied into dojo/settings/ in the docker container on startup.\nConfiguration in the UI Users with the superuser status can configure more options via the UI under Configuration / System Settings.\n","categories":"","description":"DefectDojo is highly configurable.","excerpt":"DefectDojo is highly configurable.","ref":"/django-DefectDojo/getting_started/configuration/","tags":"","title":"Configuration"},{"body":"","categories":"","description":"A lot of integrations help to fit DefectDojo in your environment","excerpt":"A lot of integrations help to fit DefectDojo in your environment","ref":"/django-DefectDojo/integrations/","tags":"","title":"Integrations"},{"body":"System-wide permissions  Administrators (aka superusers) have no limitations in the system. They can change all settings, manage users and have read and write access to all data. Staff users can add Product Types, and have access to data according to their role in a Product or Product Type. Regular users have limited functionality available. They cannot add Product Types but have access to data according to their role in a Product or Product Type  Product and Product Type permissions Users can be assigned as members to Products and Product Types, giving them one out of five predefined roles. The role defines what kind of access a user has to functions for interacting with data of that Product or Product Type:\nProduct / Product Type roles:\n    Reader Writer Maintainer Owner API Importer     Add Product Type   1) 1)    View Product Type x x x x x   Remove yourself as a member x x x x    Manage Product Type members   x x    Edit Product Type   x x    Add Product   x x    Add Product Type member as Owner    x    Delete Product Type    x            View Product x x x x x   Remove yourself as a member x x x x    Manage Product members   x x    Edit Product   x x    Add Product member as Owner    x    Delete Product    x            View Engagement x x x x x   Add Engagement  x x x    Edit Engagement  x x x    Risk Acceptance  x x x    Delete Engagement   x x            View Test x x x x x   Add Test  x x x    Edit Test  x x x    Delete Test   x x            View Finding x x x x x   Add Finding  x x x    Edit Finding  x x x    (Re-)Import Scan Result  x x x x   Delete Finding   x x            View Finding Group x x x x x   Add Finding Group  x x x    Edit Finding Group  x x x    Delete Finding Group  x x x            View Endpoint x x x x x   Add Endpoint  x x x    Edit Endpoint  x x x    Delete Endpoint   x x            Edit Benchmark  x x x    Delete Benchmark   x x            View Components x x x x x           View Note History x x x x    Add Note x x x x    Edit Note (x) 2) x x x    Delete Note (x) 2) (x) 2) x x     1) Every superuser can add Product Types. Regular users are not allowed to add Product Types, unless they are Global Owner or Maintainer.\n2) Every user is allowed to edit and delete his own notes.\nThe role of a user within a Product Type is inherited by all Products of that Product Type, unless the user is explicitly defined as a member of a Product with a different role. In that case, if a user doesn‚Äôt have a certain right for the Product Type, it is then checked if he has the right for the Product.\nA Product Type needs to have at least one owner. The last owner cannot be removed.\nGlobal permissions Users can be assigned a global role in the Edit User dialog. A global role gives a user access to all Product Types and Products, including the underlying data, with permissions according to the respective role.\nA use case for a global role could be the Chief Information Security Officer of a company who needs an overview of all systems. If he gets the global role Reader, he can see the findings for all products and also all metrics.\nSince global roles give users access to all data, only superusers are allowed to edit it.\nGroups If you have a number of users who should all have the same permissions for some Products or Product Types, you can put them together in a group. The group defines the roles for Products and Product Types that are applied to all members of the group.\nThe membership of a group itself has a role that determines what permissions the member has to manage the group:\n    Reader Maintainer Owner     Add Group 1)      View Group x x x   Remove yourself as a member x x x   Manage Group members  x x   Edit Group  x x   Add Group member as Owner   x   Delete Group   x    1) Every superuser can add groups. Regular users are not allowed to add groups.\nThe permissions to manage the roles of Products and Product types for a group is defined by the role of the user in the respective Product or Product Type.\nGroups can have a global role too. This global role gives all members of the group access to all Product Types and Products, including the underlying data, with permissions according to the respective role.\nConfiguration permissions Many configuration dialogues and API endpoints can be enabled for users or groups of users, regardless of their superuser status:\n3 configurations can still only be changed by superusers:\n System settings Notifications on system level Configuration permissions for users and groups  Warning These configuration settings are a powerful tool and should be used with great care.  ","categories":"","description":"Users have different functionality available to them, depending on their system-wide permissions and on the role they have as a member of a particular Product or Product Type.","excerpt":"Users have different functionality available to them, depending on ‚Ä¶","ref":"/django-DefectDojo/usage/permissions/","tags":"","title":"Permissions"},{"body":"Questionnaires Questionnaires provide a means for collecting test scope and deployment information from developers and respective stakeholders. DefectDojo includes functionality to create new questionnaires with custom questions, open questionnaires to receive responses for certain time periods from insiders or outsiders, and connect questionnaires with new or existing engagements.\nCreating a New Questionnaire To access, create, or modify new/existing questionnaires, navigate to the All Questionnaires dashboard from the sidebar.\nOn the questionnaire dashboard, all existing questionnaires are displayed. To quickly find a questionnaire, the filters may be used to search for snippets within the questionnaire name and/or description, as well as by active/inactive status.\nWhen questionnaires are open for responses, they will be displayed in the General Questionnaires block towards the bottom of the page.\nTo begin the process of creating a new questionnaire, select the Create Questionnaire button located in the top right of the questionnaire dashboard.\nQuestionnaires have a name and description, as well as an activity status, which are initially set on questionnaire creation, but can be modified in the future if necessary. Once these fields are filled in appropriately, the user can create the questionnaire without any questions (by selecting Create Questionnaire), or with questions (by selecting Create Questionnaire and Add Questions).\nTo add questions to a questionnaire, select the dropdown titled Select as many Questions as applicable, which will open all of the existing questions within DefectDojo. Once the desired questions are selected from the list, the dropdown can be closed, and the Update Questionnaire Questions can be selected to save the newly created questionnaire.\nNote: New questions may also be added at the time of questionnaire creation by selecting the plus located next to the questions dropdown.\nCreating New Questions The questions dashboard displays all of the questions that may exist as part of questionnaires within DefectDojo. Similar to questionnaires, to quickly find a question, the filters may be used to search for optional status, or snippets within the question name and/or description. Two types of questions exist within DefectDojo questionnaires: Text Questions and Multiple Choice Questions. To add a new question, select the Create Question button located in the top right of the questions dashboard.\nAdding Text Questions To add a text question (open-ended), fill out the add question form, where:\n Type - The type of question being created, in this case Text. Order - The order of a question describes its position in a questionnaire relative to other questions (e.g., an order of 1 will put the question higher than a question with order 4). Optional - When the optional box is checked, a question will not be required in a questionnaire. Question Text - The text that is displayed to prompt a user for their answer (e.g. What is your favorite color?).  Adding Multiple Choice Questions Similar to the process of adding a text question, choice questions (non-open-ended) allow the user to pick from a given list of choices. To add a choice question, fill out the add question form, where:\n Type - The type of question being created, in this case Choice. Order - The order of a question describes its position in a questionnaire relative to other questions (e.g., an order of 1 will put the question higher than a question with order 4). Optional - When the optional box is checked, a question will not be required in a questionnaire. Multichoice - When the multichoice box is checked, multiple choices from the list of choices may be selected by the user. Answer Choices - The possible answer choices that may be selected by a user.  Publishing a Questionnaire Once a questionnaire has been successfully created, it can be published to accept responses. To publish a questionnaire, select the plus located to the right of General Questionnaires.\nThis will prompt for a specific questionnaire to be selected, as well as a date the questionnaire response window should close. The response window sets a due date for recipients. Once these two options have been selected, publish the questionnaire by selecting Add Questionnaire.\nOnce a questionnaire is published, a link to share it can be retrieved by selecting the Share Questionnaire action. To ensure the newly created questionnaire has been constructed as expected, open the share link and view the newly created questionnaire.\nUnassigned Questionnaires When a questionnaire‚Äôs response window has closed, all of the responses will be saved, and the questionnaire will be listed as an Unassigned Answered Engagement Questionnaire on the DefectDojo dashboard.\nThere are three actions that may be taken when a questionnaire‚Äôs response window has closed: View Responses, Create Engagement, and Assign User.\nView Questionnaire Responses To view the questionnaire responses, select the View Responses action. All of the responses from the questionnaire will be displayed.\nCreate an Engagement From a Questionnaire To link the questionnaire to a product via an engagement, select the Create Engagement action. Once a product is selected from the dropdown, select Create Engagement. This will link the questionnaire results with a new engagement under the selected product, which can then be given specific details similar to other engagements in DefectDojo, such as Description, Version, Status, Tags, etc.\nTo view a questionnaire at the engagement level, navigate to the engagement linked with the desired questionnaire. Expand the Additional Features menu to reveal a Questionnaires dropdown, which will contain all of the linked questionnaires.\nAssign a Questionnaire to a User To assign a questionnaire to a user, select the Assign User action. This will prompt for a user to be selected from the dropdown of available users. Once a user is selected, assign the questionnaire to the specified user by selecting Assign Questionnaire.\nCreating Questionnaires From Engagements While questionnaires are commonly created from the questionnaire dashboard, they can also be created at the engagement level. To create a new questionnaire from within an engagement, expand the Additional Features dropdown to reveal the Questionnaires dropdown. In the right side header of the Questionnaires dropdown, select the plus to link a new questionnaire.\nOnce prompted, select a questionnaire from the available surveys list to link it with the engagement. If the user wishes to leave a response at the time of linking the questionnaire with the engagement, the Add Questionnaire and Repond option may be selected. To simply link the questionnaire with the engagement, select Add Questionnaire.\nAnonymous Questionnaires Questionnaires, by default, are only accessible by DefectDojo users. To allow outside responses to DefectDojo questionnaires, ensure the Allow Anonymous Survey Reponses option within the System Settings is selected. To share a questionnaire with anonymous users, use the questionnaire‚Äôs Share Link.\n","categories":"","description":"Collect test scope and deployment information from outsiders.","excerpt":"Collect test scope and deployment information from outsiders.","ref":"/django-DefectDojo/usage/questionnaires/","tags":"","title":"Questionnaires"},{"body":"LDAP Authentication Out of the box Defect Dojo does not support LDAP authentication.\nHowever, since Defect Dojo is built using Django, it isn‚Äôt too difficult to add support for LDAP. So long as you don‚Äôt mind building your own Docker images‚Ä¶\nWe will need to modify a grand total of 4-5 files, depending on how you want to pass Dojo your LDAP secrets.\n Dockerfile.django Dockerfile.nginx requirements.txt settings.dist.py docker-compose.yml (Optional)  Dockerfile modifications In both Dockerfile.django and Dockerfile.nginx, you want to add the following lines to the apt-get install layers:\nlibldap2-dev \\ libsasl2-dev \\ ldap-utils \\ requirements.txt Please check for the latest version of these requirements at the time of implementation on pypi.org and use those if you can.\n https://pypi.org/project/python-ldap/ https://pypi.org/project/django-auth-ldap/  Otherwise add the following to requirements.txt:\npython-ldap==3.4.2 django-auth-ldap==4.1.0 settings.dist.py Find the settings file (hint: /dojo/settings/settings.dist.py) and add the following:\nAt the top of the file:\nimport ldap from django_auth_ldap.config import LDAPSearch, GroupOfNamesType Then further down add LDAP settings to the env dict:\n# LDAP DD_LDAP_SERVER_URI=(str, 'ldap://ldap.example.com'), DD_LDAP_BIND_DN=(str, ''), DD_LDAP_BIND_PASSWORD=(str, ''), Then under the env dict add:\nAUTH_LDAP_SERVER_URI = env('DD_LDAP_SERVER_URI') AUTH_LDAP_BIND_DN = env('DD_LDAP_BIND_DN') AUTH_LDAP_BIND_PASSWORD = env('DD_LDAP_BIND_PASSWORD') AUTH_LDAP_USER_SEARCH = LDAPSearch( \"ou=Groups,dc=example,dc=com\", ldap.SCOPE_SUBTREE, \"(uid=%(user)s)\" ) AUTH_LDAP_USER_ATTR_MAP = { \"first_name\": \"givenName\", \"last_name\": \"sn\", \"email\": \"mail\", } Please make sure to customise all of the LDAP search variables to match your company‚Äôs configuration.\nFor additional group controls you can add:\n# Set up the basic group parameters. AUTH_LDAP_GROUP_SEARCH = LDAPSearch( \"dc=example,dc=com\", ldap.SCOPE_SUBTREE, \"(objectClass=groupOfNames)\", ) AUTH_LDAP_GROUP_TYPE = GroupOfNamesType(name_attr=\"cn\") # Simple group restrictions AUTH_LDAP_REQUIRE_GROUP = \"cn=DD_USER_ACTIVE,ou=Groups,dc=example,dc=com\" AUTH_LDAP_USER_FLAGS_BY_GROUP = { \"is_active\": \"cn=DD_USER_ACTIVE,ou=Groups,dc=example,dc=com\", \"is_staff\": \"cn=DD_USER_STAFF,ou=Groups,dc=example,dc=com\", \"is_superuser\": \"cn=DD_USER_ADMIN,ou=Groups,dc=example,dc=com\", } Then also add 'django_auth_ldap.backend.LDAPBackend' to the AUTHENTICATION_BACKENDS variable, for example:\nAUTHENTICATION_BACKENDS = ( 'django_auth_ldap.backend.LDAPBackend', 'django.contrib.auth.backends.RemoteUserBackend', 'django.contrib.auth.backends.ModelBackend', ) Read the docs for Django Authentication with LDAP here: https://django-auth-ldap.readthedocs.io/en/latest/\ndocker-compose.yml In order to pass the variables to the settings.dist.py file via docker, it‚Äôs a good idea to add these to the docker-compose file.\nYou can do this by adding the following variables to the environment section for the uwsgi image:\nDD_LDAP_SERVER_URI: \"${DD_LDAP_SERVER_URI:-ldap://ldap.example.com}\" DD_LDAP_BIND_DN: \"${DD_LDAP_BIND_DN:-}\" DD_LDAP_BIND_PASSWORD: \"${DD_LDAP_BIND_PASSWORD:-}\" Alternatively you can set these values in a local_settings.py file.\n","categories":"","description":"Authenticate users using LDAP","excerpt":"Authenticate users using LDAP","ref":"/django-DefectDojo/integrations/ldap-authentication/","tags":"","title":"Authentication via LDAP"},{"body":"You can find further information in the contributing guidelines.\n","categories":"","description":"How you can help to make DefectDojo even better","excerpt":"How you can help to make DefectDojo even better","ref":"/django-DefectDojo/contributing/","tags":"","title":"Contributing"},{"body":"Example 1 - Bill the security engineer Bill wants a place to keep track of what he's worked on, so that he can show his boss exactly what issues he reports, and statistics about how long it takes to close them.\nWhen he is asked to audit an application, Bill registers a new Product in DefectDojo, and creates a new Engagement. Here he sets some basic information, like how long he expects the Engagement will take, who will be leading the testing (himself), what Product he will be working on, and what tests he will be doing.\nNext, he can add a Test to the Engagement, or upload a Nessus scan and start picking out the real vulnerabilities from the false positives (Nessus scan Findings are imported as inactive by default).\nWithin the Test section, Bill can add Findings for any issues that he has uncovered during his audit. He can assign a severity to the Findings, describe replication steps, mitigation strategies, and impact on the system. This will come in handy when he wants to generate a report to send to the development team responsible for this Product, or his manager.\nOnce Bill has completed his Engagement, he can close the Engagement on the main Engagement page. He can then view the results of his Tests, and generate a report to send to the development team.\nIf Bill hears back from the development team that they won't be able to fix the issue for a while, he can make a note of this on the Engagement page. Bill will also receive Alerts for any bugs that persist longer than they are supposed to based on their severity.\nExample 2 - John the QE manager John wants to keep tabs on what his team members are up to, and find issues that are taking a long time to get fixed. He creates his own DefectDojo account with superuser privileges so that he can view other team members' metrics.\nTo get a better idea of what his team members are currently working on, he can start by checking the Calendar. This will show him any active Engagements that his team is involved in, based on the dates assigned to those Engagements.\nHe can view metrics for a Product Type, such as \"Third Party Apps\" to track his team's activity and follow up with Product teams who have long-lived bugs. He can also look at all the Findings for which there is a Risk Acceptance associated, and ensure that the proper documentation or timeline has been provided for the Findings in question.\nIf he wants to check on a particular team member's progress, he can look at the Engineer Metrics dashboard under \"Additional Metrics\" for that user.\n","categories":"","description":"Two examples how DefectDojo can be used in day-to-day operations.","excerpt":"Two examples how DefectDojo can be used in day-to-day operations.","ref":"/django-DefectDojo/usage/workflows/","tags":"","title":"Example workflows"},{"body":"DefectDojo's JIRA integration is bidirectional. You may push findings to JIRA and share comments. If an issue is closed in JIRA it will automatically be closed in Dojo.\nNOTE: These steps will configure the necessary webhook in JIRA and add JIRA integration into DefectDojo. This isn't sufficient by itself, you will need to configure products and findings to push to JIRA. On a product's settings page you will need to define a:\n Project Key (and this project must exist in JIRA) JIRA Configuration (select the JIRA configuration that you create in the steps below) Component (can be left blank)  Then elect (via tickbox) whether you want to 'Push all issues', 'Enable engagement epic mapping' and/or 'Push notes'. Then click on 'Submit'.\nIf creating a Finding, ensure to tick 'Push to jira' if desired.\nEnabling the Webhook  Visit https://\u003cYOUR JIRA URL\u003e/plugins/servlet/webhooks Click 'Create a Webhook' For the field labeled 'URL' enter: https://\u003cYOUR DOJO DOMAIN\u003e/jira/webhook/\u003cYOUR GENERATED WEBHOOK SECRET\u003e This value can be found under Defect Dojo System settings Under 'Comments' enable 'Created'. Under Issue enable 'Updated'.  Configurations in Dojo  Navigate to the System Settings from the menu on the left side or by directly visiting \u003cyour url\u003e/system_settings. Enable 'Enable JIRA integration' and click submit. For the webhook created in Enabling the Webhook, enable 'Enable JIRA web hook' and click submit.  Adding JIRA to Dojo   Click 'JIRA' from the left hand menu.\n  Select 'Add Configuration' from the drop-down.\n  For JIRA Server:\nEnter the Username \u0026 Password. A Username and JIRA Personal Access Token will not necessarily work.\nFor JIRA Cloud:\nEnter Email Address \u0026 API token for Jira\n  To obtain the 'open status key' and 'closed status key' visit https://\u003cYOUR JIRA URL\u003e/rest/api/latest/issue/\u003cANY VALID ISSUE KEY\u003e/transitions?expand=transitions.fields\n  The 'id' for 'Todo' should be filled in as the 'open status key'\n  The 'id' for 'Done' should be filled in as the 'closed status key'\n  To obtain 'epic name id': If you have admin access to JIRA:\n visit: https://\u003cYOUR JIRA URL\u003e/secure/admin/ViewCustomFields.jspa Click on the cog next to 'Epic Name' and select view. The numeric value for 'epic name id' will be displayed in the URL Note: dojojira uses the same celery functionality as reports. Make sure the celery runner is setup correctly as described: https://documentation.defectdojo.com/basics/features/#reports  Or\n login to JIRA visit https://yourjiraurl/rest/api/2/field and use control+F or grep to search for 'Epic Name' it should look something like this:  { ‚Äúid‚Äù:‚Äúcustomfield_122‚Äù, ‚Äúkey‚Äù:‚Äúcustomfield_122‚Äù, ‚Äúname‚Äù:‚ÄúEpic Name‚Äù, ‚Äúcustom‚Äù:true, ‚Äúorderable‚Äù:true, ‚Äúnavigable‚Äù:true, ‚Äúsearchable‚Äù:true, ‚ÄúclauseNames‚Äù:‚Äúcf[122]‚Äù, ‚ÄúEpic Name‚Äù], ‚Äúschema‚Äù:{‚Äútype‚Äù:‚Äústring‚Äù,‚Äúcustom‚Äù:‚Äúcom.pyxis.greenhopper.jira:gh-epic-label‚Äù,‚ÄúcustomId‚Äù:122} }\nIn the above example 122 is the number needed\nCustomize JIRA issue description By default Defect Dojo uses the dojo/templates/issue-trackers/jira_full/jira-description.tpl template to render the description of the ‚Äòto be‚Äô created JIRA issue. This file can be modified to your needs, rebuild all containers afterwards. There‚Äôs also a more limited template available, which can be chosen when configuring a JIRA Instance or JIRA Project for a Product or Engagement:\nAny folder added to dojo/templates/issue-trackers/ will be added to the dropdown (after rebuilding/restarting the containers).\nEngagement Epic Mapping If creating an Engagement, ensure to tick ‚ÄòEnable engagement epic mapping‚Äô if desired. This can also be done after engagement creation on the edit engagement page. This will create an ‚ÄòEpic‚Äô type issue within Jira. All findings in the engagement pushed to Jira will have a link to this Epic issue. If Epic Mapping was enabled after associated findings have already been pushed to Jira, simply pushing them again will link the Jira issue to the Epic issue.\nPushing findings Findings can be pushed to Jira in a number of ways:\n When importing scanner reports, select ‚ÄòPush to JIRA‚Äô to push every single finding in the report to Jira When creating a new finding, select ‚ÄòPush to JIRA‚Äô and submit. This will create the finding in DefectDojo and Jira simultaneously If a finding already exist, visit the edit finding page and find the ‚ÄòPush to JIRA‚Äô tick box at the bottom When viewing a list of findings, select each relevant tick boxes to the left of the finding, and click the ‚ÄòBulk Edit‚Äô button at the top. find ‚ÄòPush to JIRA‚Äô at the bottom of the menu  Status Sync DefectDojo will try to keep the status in sync with the status in JIRA using the Close and Reopen transition IDs configured for each JIRA instance. This will only work if your workflow in JIRA allows the Close transition to be performed from every status a JIRA issue can be in.\nKnown Issues The Risk Acceptance feature in DefectDojo will (for that reason) not (yet) try to sync statuses. A comment will be pushed to JIRA if a finding is risk accepted or unaccepted. Contributions are welcome to enhance the integration.\nStatus reconciliation Sometimes JIRA is down, or Defect Dojo is down, or there was bug in a webhook. In this case JIRA can become out of sync with Defect Dojo. If this is the case for lots of issues, manual reconciliation might not be feasible. For this scenario there is the management command ‚Äòjira_status_reconciliation‚Äô.\nusage: manage.py jira_status_reconciliation [-h] [--mode MODE] [--product PRODUCT] [--engagement ENGAGEMENT] [--dryrun] [--version] [-v {0,1,2,3}] Reconcile finding status with JIRA issue status, stdout will contain semicolon seperated CSV results. Risk Accepted findings are skipped. Findings created before 1.14.0 are skipped. optional arguments: -h, --help show this help message and exit --mode MODE - reconcile: (default)reconcile any differences in status between Defect Dojo and JIRA, will look at the latest status change timestamp in both systems to determine which one is the correct status - push_status_to_jira: update JIRA status for all JIRA issues connected to a Defect Dojo finding (will not push summary/description, only status) - import_status_from_jira: update Defect Dojo finding status from JIRA --product PRODUCT Only process findings in this product (name) --engagement ENGAGEMENT Only process findings in this product (name) --dryrun Only print actions to be performed, but make no modifications. -v {0,1,2,3}, --verbosity {0,1,2,3} Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output This can be executed from the uwsgi docker container using:\n$ docker-compose exec uwsgi /bin/bash -c 'python manage.py jira_status_reconciliation' DEBUG output can be obtains via -v 3, but only after increasing the logging to DEBUG level in your settings.dist.py or local_settings.py file\n$ docker-compose exec uwsgi /bin/bash -c 'python manage.py jira_status_reconciliation -v 3' At the end of the command a semicolon seperated CSV summary will be printed. This can be captured by redirecting stdout to a file:\n$ docker-compose exec uwsgi /bin/bash -c 'python manage.py jira_status_reconciliation \u003e jira_reconciliation.csv' Troubleshooting JIRA integration JIRA actions are typically performed in the celery background process. Errors are logged as alerts/notifications to be seen on the top right of the DefectDojo UI and in stdout of the celery workers.\n","categories":"","description":"Bidirectional integration of DefectDojo findings with Jira issues.","excerpt":"Bidirectional integration of DefectDojo findings with Jira issues.","ref":"/django-DefectDojo/integrations/jira/","tags":"","title":"JIRA integration"},{"body":"Asynchronous Import DefectDojo offers an experimental feature to aynschronously import security reports. This feature works in most use cases, but struggles when doing things such as pushing to Jira during the import process. Because Endpoints are still being processed and created even after the import procedure is completed, pushing Findings to Jira can result in incomplete Jira tickets. It is advised to wait until after import has been completed 100%.\nTo enable this feature, set ASYNC_FINDING_IMPORT to True in local_settings.py\nAsynchronous Delete For larger instances, deleting an object can take minutes for all related objects to be expanded into memory, rendered on the page, and then removing all objects from the database. To combat this issue, two settings can be set in local_settings.py:\nASYNC_OBJECT_DELETE Deleting an object asynchronously changes the way an object is deleted under the hood. By removing the need to expand into memory, a lot of time (and memory) can be saved by offloading the lookups and removals onto celery processes. This process works by starting at the bottom of a given object, and walking the tree upwards rather than downwards. This way, objects can be seperated into buckets, and then deleted.\nDELETE_PREVIEW Previewing all the objects to be deleted takes almost as much time as deleting the objects itself. This is a safety feature intended to warn users of what they are about to delete, as well as educating users of how the delete functionality works by cascade deleting all related objects. With this feature enabled, the user will only see the following text in the delete preview (without any database lookups)\nPreviewing the relationships has been disabled.\n","categories":"","description":"Settings to configure to enhance performance in DefectDojo","excerpt":"Settings to configure to enhance performance in DefectDojo","ref":"/django-DefectDojo/usage/performance/","tags":"","title":"Performance Enhancements"},{"body":"Production with docker-compose The docker-compose.yml file in this repository is fully functional to evaluate DefectDojo in your local environment.\nAlthough Docker Compose is one of the supported installation methods to deploy a containerized DefectDojo in a production environment, the docker-compose.yml file is not intended for production use without first customizing it to your particular situation.\nSee Running with Docker Compose for more information how to run DefectDojo with Docker Compose.\nDatabase performance and backup It is recommended to use a dedicated database server and not the preconfigured MySQL database. This will improve the performance of DefectDojo\nIn both case, if you use a dedicated database server or if you should decide to use the preconfigured MySQL database, make sure to make regular backups of the data. For a dedicated database server follow the instructions that come with the database server. For the preconfigured MySQL you can use mysqldump, e.g. as described in How to backup a Docker MySQL database.\nBackup of Media files Media files for uploaded files, including threat models and risk acceptance, are stored in a docker volume. This volume needs to be backed up regularly.\nInstance size Information Please read the paragraphs below about key processes tweaks.  Having taken the database to run elsewhere, the minimum recommendation is:\n 2 vCPUs 8 GB of RAM 2 GB of disk space (remember, your database is not here -- so basically, what you have for your O/S should do). You could allocate a different disk than your OS's for potential performance improvements.  Key processes Per https://github.com/DefectDojo/django-DefectDojo/pull/2813, it is now easy to somewhat improve the uWSGI and celery worker performance.\nuWSGI By default (except in ptvsd mode for debug purposes), uWSGI will handle 4 concurrent connections.\nBased on your resource settings, you can tweak:\n DD_UWSGI_NUM_OF_PROCESSES for the number of spawned processes. (default 2) DD_UWSGI_NUM_OF_THREADS for the number of threads in these processes. (default 2)  For example, you may have 4 processes with 6 threads each, yielding 24 concurrent connections.\nCelery worker By default, a single mono-process celery worker is spawned. This is fine until you start having many findings, and when async operations like deduplication start to kick in. Eventually, it will starve your resources and crawl to a halt, while operations continue to queue up.\nThe following variables will help a lot, while keeping a single celery worker container.\n DD_CELERY_WORKER_POOL_TYPE will let you switch to prefork. (default solo)  As you've enabled prefork, the following variables have to be used. The default are working fairly well, see the Dockerfile.django for in-file references.\n DD_CELERY_WORKER_AUTOSCALE_MIN defaults to 2. DD_CELERY_WORKER_AUTOSCALE_MAX defaults to 8. DD_CELERY_WORKER_CONCURRENCY defaults to 8. DD_CELERY_WORKER_PREFETCH_MULTIPLIER defaults to 128.  You can execute the following command to see the configuration:\ndocker-compose exec celerybeat bash -c \"celery -A dojo inspect stats\" and see what is in effect.\nAsynchronous Imports This is an experimental features that has some concerns that need to be addressed before it can be used reliably.\nImport and Re-Import can also be configured to handle uploads asynchronously to aid in importing especially large files. It works by batching Findings and Endpoints by a configurable amount. Each batch will be be processed in seperate celery tasks.\nThe following variables have to be used.\n DD_ASYNC_FINDING_IMPORT defaults to False DD_ASYNC_FINDING_IMPORT_CHUNK_SIZE deafults to 100  When using asynchronous imports with dynamic scanners, Endpoints will continue to ‚Äútrickle‚Äù in even after the import has returned a successful respsonse. This is becasue processing continues to occur after the Findings have already been imported.\nTo determine if an import has been fully completed, please see the progress bar in the appropriate test.\nMonitoring To expose Django statistics for Prometheus, set DJANGO_METRICS_ENABLED to True in the settings (see Configuration).\nThe Prometheus endpoint is than available under the path: http://dd_server/django_metrics/metrics\n","categories":"","description":"Productive use of DefectDojo needs consideration of performance and backups.","excerpt":"Productive use of DefectDojo needs consideration of performance and ‚Ä¶","ref":"/django-DefectDojo/getting_started/running-in-production/","tags":"","title":"Running in production"},{"body":"Findings can have a filepath and a line number as the location of the vulnerability. This is typically set when scanning an application with a Static Application Security Test (SAST) tool. If the repository of the source code is specified in the Engagement, DefectDojo will present the filepath as a link and the user can navigate directly to the location of the vulnerability.\nSetting the repository in the Engagement While editing the Engagement, users can set the URL of the repo. It needs to be the URL including the branch, e.g. https://github.com/DefectDojo/django-DefectDojo/tree/dev (GitHub) or https://gitlab.com/gitlab-org/gitlab/-/tree/master (GitLab).\nLink in Finding When viewing a finding, the location will be presented as a link, if the repository of the source code has been set in the Engagement:\nClicking on this link will open a new tab in the browser, with the source file of the vulnerability at the corresponding line number:\n","categories":"","description":"Integration of repositories to navigate to the locaction of findings in the source code.","excerpt":"Integration of repositories to navigate to the locaction of findings ‚Ä¶","ref":"/django-DefectDojo/integrations/source-code-repositories/","tags":"","title":"Source code repositories"},{"body":"Docker-compose When you deploy a vanilla docker-compose, it will create a persistent volume for your MySQL database. As long as your volume is there, you should not lose any data.\nUsing docker images provided in DockerHub Information If you're using latest, then you need to pre pull the latest from DockerHub to update.  The generic upgrade method for docker-compose follows these steps:\n  Pull the latest version\ndocker pull defectdojo/defectdojo-django:latest docker pull defectdojo/defectdojo-nginx:latest   If you would like to use something older (so not the latest version), specify the version (tag) you want to upgrade to:\ndocker pull defectdojo/defectdojo-django:1.10.2 docker pull defectdojo/defectdojo-nginx:1.10.2   Go to the directory where your docker-compose.yml file lives\n  Stop DefectDojo: ./dc-stop.sh\n  Re-start DefectDojo, allowing for container recreation: ./dc-up-d.sh\n  Database migrations will be run automatically by the initializer. Check the output via docker-compose logs initializer or relevant k8s command\n  If you have the initializer disabled (or if you want to be on the safe side), run the migration command: docker-compose exec uwsgi /bin/bash -c 'python manage.py migrate\n  Building your local images If you build your images locally and do not use the ones from DockerHub, the instructions are much the same, except that you‚Äôd build your images first. (Of course, if you‚Äôre doing this, then you know you have to update the source code first)\nReplace the first step above with this one: docker-compose build\ngodojo installations If you have installed DefectDojo on ‚Äúiron‚Äù and wish to upgrade the installation, please see the instructions in the repo.\nUpgrading to DefectDojo Version 2.18.x Upgrade instructions for helm chart with rabbitMQ enabled: The rabbitMQ uses a statefulset by default. Before upgrading the helm chart we have to ensure that all queues are empty:\nkubectl exec -i \u003cname_of_the_rabbitmq_pod\u003e -- rabbitmqctl list_queues Next step is to delete rabbitMQ pvc:\nkubectl delete pvc -l app.kubernetes.io/name=rabbitmq Last step is to perform the upgrade.\nFor more information: https://artifacthub.io/packages/helm/bitnami/rabbitmq/11.2.0\nUpgrading to DefectDojo Version 2.17.x. There are no special instruction for upgrading to 2.17.0. Check the Release Notes for the contents of the release.\nUpgrading to DefectDojo Version 2.16.x. There are no special instruction for upgrading to 2.16.0. Check the Release Notes for the contents of the release.\nUpgrading to DefectDojo Version 2.15.x. There are no special instruction for upgrading to 2.15.0. Check the Release Notes for the contents of the release.\nUpgrading to DefectDojo Version 2.13.x. The last release implemented the search for vulnerability ids, but the search database was not initialized. To populate the database table of the vulnerability ids, execute this django command from the defect dojo installation directory or from a shell of the Docker container or Kubernetes pod:\n./manage.py migrate_cve\nAdditionally this requires a one-time rebuild of the Django-Watson search index. Execute this django command from the defect dojo installation directory or from a shell of the Docker container or Kubernetes pod:\n./manage.py buildwatson\nUpgrade instructions for helm chart with postgres enabled: The postgres database uses a statefulset by default. Before upgrading the helm chart we have to delete the statefullset and ensure that the pvc is reused, to keep the data. For more information: https://docs.bitnami.com/kubernetes/infrastructure/postgresql/administration/upgrade/ .\nhelm repo update helm dependency update ./helm/defectdojo # obtain name oft the postgres pvc export POSTGRESQL_PVC=$(kubectl get pvc -l app.kubernetes.io/instance=defectdojo,role=primary -o jsonpath=\"{.items[0].metadata.name}\") # delete postgres statefulset kubectl delete statefulsets.apps defectdojo-postgresql --namespace default --cascade=orphan # upgrade helm upgrade \\  defectdojo \\  ./helm/defectdojo/ \\  --set primary.persistence.existingClaim=$POSTGRESQL_PVC \\  ... # add your custom settings Further changes:\nLegacy authorization for changing configurations based on staff users has been removed.\nUpgrading to DefectDojo Version 2.12.x. Breaking change for search: The field cve has been removed from the search index for Findings and the Vulnerability Ids have been added to the search index. With this the syntax to search explicitly for vulnerability ids have been changed from cve: to vulnerability_id:, e.g. vulnerability_id:CVE-2020-27619.\nUpgrading to DefectDojo Version 2.10.x. Breaking change for Findings: The field cve will be replaced by a list of Vulnerability Ids, which can store references to security advisories associated with this finding. These can be Common Vulnerabilities and Exposures (CVE) or from other sources, eg. GitHub Security Advisories. Although the field does still exist in the code, the API and the UI have already been changed to use the list of Vulnerability Ids. Other areas like hash code calculation, search and parsers will be migrated step by step in later stages.\nThis change also causes an API change for the endpoint /engagements/{id}/accept_risks/.\nUpgrading to DefectDojo Version 2.9.x. Breaking change for APIv2: configuration_url was removed from API endpoint /api/v2/tool_configurations/ due to redundancy.\nUpgrading to DefectDojo Version 2.8.x. Breaking change for Docker Compose: Starting DefectDojo with Docker Compose now supports 2 databases (MySQL and PostgreSQL) and 2 celery brokers (RabbitMQ and Redis). To make this possible, docker-compose needs to be started with the parameters --profile and --env-file. You can get more information in Setup via Docker Compose - Profiles. The profile mysql-rabbitmq provides the same configuration as in previous releases. With this the prerequisites have changed as well: Docker requires at least version 19.03.0 and Docker Compose 1.28.0.\nBreaking change for Helm Chart: In one of the last releases we upgraded the redis dependency in our helm chart without renaming keys in our helm chart. We fixed this bug with this release, but you may want to check if all redis values are correct (Pull Request).\nThe flexible permissions for the configuration of DefectDojo are now active by default. With this, the flag Staff for users is not relevant and not visible anymore. The old behaviour can still be activated by setting the parameter FEATURE_CONFIGURATION_AUTHORIZATION to False. If you haven‚Äôt done so with the previous release, you can still run a migration script with ./manage.py migrate_staff_users. This script:\n creates a group for all staff users, sets all configuration permissions that staff users had and sets the global Owner role, if AUTHORIZATION_STAFF_OVERRIDE is set to True.  Upgrading to DefectDojo Version 2.7.x. This release is a breaking change regarding the Choctaw Hog parser. As the maintainers of this project unified multiple parsers under the RustyHog parser, we now support the parsing of Choctaw Hog JSON output files through the Rusty Hog parser. Furthermore, we also support Gottingen Hog and Essex Hog JSON output files with the RustyHog parser.\nThere is another breaking change regarding the import of SSLyze scans. The parser has been renamed from SSLyze 3 Scan (JSON) to SSLyze Scan (JSON). The data in the database is fixed by the initializer, but it may break scripted API calls.\nRelease 2.7.0 contains a beta functionality to make permissions for the configuration of DefectDojo more flexible. When the settings parameter FEATURE_CONFIGURATION_AUTHORIZATION is set to True, many configuration dialogues and API endpoints can be enabled for users or groups of users, regardless of their Superuser or Staff status, see Configuration Permissions.\nThe functionality using the flag AUTHORIZATION_STAFF_OVERRIDE has been removed. The same result can be achieved with giving the staff users a global Owner role.\nTo support the transition for these 2 changes, you can run a migration script with ./manage.py migrate_staff_users. This script:\n creates a group for all staff users, sets all configuration permissions that staff users had and sets the global Owner role, if AUTHORIZATION_STAFF_OVERRIDE is set to True.  Upgrading to DefectDojo Version 2.6.x. There are no special instruction for upgrading to 2.6.0. Check the Release Notes for the contents of the release.\nPlease consult the security advisories GHSA-f82x-m585-gj24 (moderate) and GHSA-v7fv-g69g-x7p2 (high) to see what security issues were fixed in this release. These will be published and become visible at January 18th, 2022.\nUpgrading to DefectDojo Version 2.5.x. Legacy authorization has been completely removed with version 2.5.0. This includes removal of the migration of users to the new authorization as described in https://documentation.defectdojo.com/getting_started/upgrading/#authorization. If you are still using the legacy authorization, you should run the migration with ./manage.py migrate_authorization_v2 before upgrading to version 2.5.0\nThis release introduces the ‚ÄúForgot password‚Äù functionality (DD_FORGOT_PASSWORD: default True). The function allows sending an e-mail with the reset password link. Missing configuration or misconfiguration of SMTP (DD_EMAIL_URL) could raise an error (HTTP-500). Check and test (for example by resetting your own password) if you configured SMTP correctly. If you want to avoid HTTP-500 and you don‚Äôt want to set up SMTP, you can just simply switch off the ‚ÄúForgot password‚Äù functionality (DD_FORGOT_PASSWORD=False).\nRelease renamed system setting mail_notifications_from to email_from. This value will not be used only for sending notifications but also for sending the reset password emails. It is highly recommended to check the content of this value if you are satisfied. If you installed DefectDojo earlier, you can expect \"from@example.com\" there. A fresh installation will use \"no-reply@example.com\"\nThis release updates our helm dependencies. There is a breaking change if you are using the mysql database from the helm chart because we replaced the deprecated chart from the stable repo with a chart from bitnami. If you have persistance enabled, ensure to backup your data before upgrading. All data get lost when replacing the mysql chart during the upgrade. For data migration take a look at the mysql backup and restore process.\nFurthermore we updated our kubernetes version. Current tests run on 1.18.16 and 1.22.0.\nUpgrading to DefectDojo Version 2.4.x. (Security Release) This releases fixes a High severity vulnerability for which the details will be disclosed on November 16th in GHSA-fwg9-752c-qh8w\nThere is a breaking change in the API for importing and re-importings scans with SonarQube API and Cobalt.io API. The scan configurations have been unified and are set now with the attribute api_scan_configuration. The existing configurations for SonarQube API and Cobalt.io API have been migrated.\nAt the request of pyup.io, we had to remove the parser for Safety scans.\nUpgrading to DefectDojo Version 2.3.x. There are no special instruction for upgrading to 2.3.0. In 2.3.0 we changed the default password hashing algorithm to Argon2 (from PBKDF2). When logging in, exising hashes get replaced by an Argon2 hash. If you want to rehash password without users having to login, please see the Django password management docs. The previous password hashing algorithm (PBKDF2) was not unsafe, but we wanted to follow the OWASP guidelines.\nUpgrading to DefectDojo Version 2.2.x. Upgrade to 2.0.0 contained migration of endpoints. Some parts of migration haven‚Äôt been done properly. This deficiency may manifest as a doubled slash in endpoint URLs (like http://foo.bar:8080//test) or as a problem with deduplication of the same endpoints. The mentioned bug was fixed in 2.2.0 and if you have seen these kinds of problems, just rerun ‚ÄúEndpoint migration‚Äù as it is written in Upgrading to DefectDojo Version 2.0.x..\nUpgrading to DefectDojo Version 2.0.x. Follow the usual steps to upgrade as described above.\nBEFORE UPGRADING\n If you are using SAML2 checkout the new documentaion and update you settings following the migration section. We replaced django-saml2-auth with djangosaml2.  AFTER UPGRADING\n Usual migration process (python manage.py migrate) try to migrate all endpoints to new format and merge duplicates. All broken endpoints (which weren‚Äôt possible to migrate) have red flag üö© in standard list of endpoints. Check if all your endpoints was migrated successfully, go to: https:///endpoint/migrate. Alternatively, this can be run as management command: docker-compose exec uwsgi ./manage.py endpoint_migration --dry-run When all endpoint will be fixed (there is not broken endpoint), press ‚ÄúRun migration‚Äù in https:///endpoint/migrate Or, you can run management command: docker-compose exec uwsgi ./manage.py endpoint_migration Details about endpoint migration / improvements in https://github.com/DefectDojo/django-DefectDojo/pull/4473  We decided to name this version 2.0.0 because we did some big cleanups in this release:\n  Remove API v1 (#4413)\n  Remove setup.bash installation method (#4417)\n  Rename Finding.is_Mitigated field to Finding.is_mitigated (#3854)\n  Remove everything related to the old tagging library (#4419)\n  Remove S0/S1/S2../S5 severity display option (#4415)\n  Refactor EndPoint handling/formatting (#4473)\n  Upgrade to Django 3.x (#3632)\n  PDF Reports removed (#4418)\n  Hashcode calculation logic has changed. To update existing findings run:\n./manage.py dedupe --hash_code_only.\n  If you‚Äôre using docker:\ndocker-compose exec uwsgi ./manage.py dedupe --hash_code_only.\nThis can take a while depending on your instance size.\n See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/2.0.0  Endpoints  The usual migration process (python manage.py migrate) tries to migrate all endpoints to new format and merge duplicates. All broken endpoints (which weren‚Äôt possible to migrate) have a red flag üö© in the standard list of endpoints. Check if all your endpoints were migrated successfully, go to: https:///endpoint/migrate. Alternatively, this can be run as management command: docker-compose exec uwsgi ./manage.py endpoint_migration --dry-run When all endpoint are fixed (there is not broken endpoint), press ‚ÄúRun migration‚Äù in https:///endpoint/migrate Or, you can run management command: docker-compose exec uwsgi ./manage.py endpoint_migration Details about endpoint migration / improvements in https://github.com/DefectDojo/django-DefectDojo/pull/4473  Authorization The new authorization system for Products and Product Types based on roles is the default now. The fields for authorized users are not available anymore, but you can assign roles as described in Permissions. Users are migrated automatically, so that their permissions are as close as possible to the previous authorization:\n Superusers will still have all permissions on Products and Product Types, so they must not be changed. Staff users have had all permissions for all product types and products, so they will be get a global role as Owner. Product_Members and Product Type_Members will be added for authorized users according to the settings for the previous authorization:  The Reader role is set as the default. If AUTHORIZED_USERS_ALLOW_STAFF is True, the user will get the Owner role for the respective Product or Product Type. If AUTHORIZED_USERS_ALLOW_CHANGE or AUTHORIZED_USERS_ALLOW_DELETE is True, the user will get the Writer role for the respective Product or Product Type.    The new authorization is active for both UI and API. Permissions set via authorized users or via the Django Admin interface are no longer taken into account.\nPlease review the roles for your users after the upgrade to avoid an unintended permissions creep.\nUpgrading to DefectDojo Version 1.15.x   See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.15.0\n  If you have made changes to JIRA templates or the template config in the JIRA Project config for instances/products/engagements: The jira template settings introduced in 1.13 have been changed. You now have to select a subfolder instead of a sinlge template file. If you have chosen a non-default template here, you have to reapply that to all products / engagements. Also you have to move your custom templates into the correct subfolder in dojo/templates/issue-trackers/.\n  Hashcode calculation logic has changed in #4134, #4308 and #4310 to update existing findings run:\n./manage.py dedupe --hash_code_only\n  If you‚Äôre using docker:\ndocker-compose exec uwsgi ./manage.py dedupe --hash_code_only\nThis can take a while depending on your instance size.\nUpgrading to DefectDojo Version 1.14.x  See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.14.0  Note that the below fields are now optional without default value. They will not be filled anymore with values such as ‚ÄúNo references given‚Äù when found empty while saving the findings\n mitigation references impact url  Upgrading to DefectDojo Version 1.13.x   See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.13.0\n  Hashcode settings affecting deduplication have changed, to update existing findings run:\n./manage.py dedupe\n  If you‚Äôre using docker:\ndocker-compose exec uwsgi ./manage.py dedupe  This can take a while depeneding on your instance size. It might possible that new duplicates are detected among existing findings, so make a backup before running!\nUpgrading to DefectDojo Version 1.12.x  See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.12.0 1.12.1 is a security release https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.12.1  Upgrading to DefectDojo Version 1.11.x  See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.11.0 1.11.1 is a security release https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.11.1  Upgrading to DefectDojo Version 1.10.x 1.10.4 is a security release\n See the security advisory: https://github.com/DefectDojo/django-DefectDojo/security/advisories/GHSA-96vq-gqr9-vf2c See release notes: https://github.com/DefectDojo/django-DefectDojo/releases/tag/1.10.4 Version 1.10.4 replaces 1.10.3 as the latter contained an incomplete fix  What's New:\n See release notes: https://github.com/DefectDojo/django-DefectDojo/releases DefectDojo now provides a settings.py file out-of-the-box. Custom settings need to go into local\\_settings.py. See https://github.com/DefectDojo/django-DefectDojo/blob/master/dojo/settings/settings.py and https://github.com/DefectDojo/django-DefectDojo/blob/master/docker/extra_settings/README.md A quickfix is to rename your own / customized settings.py or settings.dist.py to local\\_settings.py. Details of that PR: https://github.com/DefectDojo/django-DefectDojo/pull/3136 Major JIRA integration refactoring, for which you should at least use 1.10.1 and not 1.10.0 for many bug fixes.  Breaking changes\nKubernetes/Helm users: we have moved away from the \"stable\" repository to \"bitnami\" in this release. The bitnami postgresql chart required us to add a new key to the postgresql secret, which will give you the error postgresql-postgres-password is missing if you have createPostgresqlSecret: false. In 1.10.1, a fix was also included to allow your existing postgresqlPassword to be reused properly.\nIncluding in 1.10.1 were a couple fixes related to a rabbitMQ upgrade. The path to access password, erlangCookie and existingPasswordSecret changed from rabbitmq to auth. Furthermore, as rabbitMQ is deployed as a StatefulSet, an in-place upgrade is not possible and an error will likely be thrown such as Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden. After ensuring your rabbitMQ celery queue is empty, you will then want to delete your rabbitMQ StatefulSet and PVC to allow them to get re-created, or fully delete and recreate defectdojo.\nUpgrading to DefectDojo Version 1.9.3 This is a security release\n See the security advisory See release notes  What's New:\n See release notes: https://github.com/DefectDojo/django-DefectDojo/releases  NOTE:\nWhen upgrading from before 1.9.2, a corrective script may need to be ran\n./manage.py create\\_endpoint\\_status\nIf you're using docker:\ndocker-compose exec uwsgi ./manage.py create\\_endpoint\\_status\nThis can take a while depending on your hardware and the number of findings in your instance.\n Search index tweaking index rebuild after upgrade:  This requires a (one-time) rebuild of the Django-Watson search index. Execute the django command from the defect dojo installation directory:\n./manage.py buildwatson]\nIf you're using docker:\ndocker-compose exec uwsgi ./manage.py buildwatson\nThis can take a while depending on your hardware and the number of findings in your instance.\nUpgrading to DefectDojo Version 1.8.0 What's New:\n See release notes: https://github.com/DefectDojo/django-DefectDojo/releases Improved search, which requires an index rebuild (https://github.com/DefectDojo/django-DefectDojo/pull/2861)  This requires a (one-time) rebuild of the Django-Watson search index. Execute the django command from the defect dojo installation directory:\n./manage.py buildwatson\nIf you're using docker:\ndocker-compose exec uwsgi ./manage.py buildwatson\nThis can take a while depending on your hardware and the number of findings in your instance.\n NOTE:  As a result of a breaking bug revolving around Endpoint_status objects, a corrective script will need to be ran after every dynamic scan imported through either API version.\nThe script can be found here\n./manage.py create\\_endpoint\\_status\nIf you're using docker:\ndocker-compose exec uwsgi ./manage.py create\\_endpoint\\_status\nThis can take a while depending on your hardware and the number of findings in your instance.\nUpgrading to DefectDojo Version 1.7.0 What's New:\n Updated search, you can now search for CVE-XXXX-YYYY Updated search index, fields added to index: 'id', 'title', 'cve', 'url', 'severity', 'description', 'mitigation', 'impact', 'steps_to_reproduce', 'severity_justification', 'references', 'sourcefilepath', 'sourcefile', 'hash_code', 'file_path', 'component_name', 'component_version', 'unique_id_from_tool'  This requires a (one-time) rebuild of the Django-Watson search index. Execute the django command from the defect dojo installation directory:\n./manage.py buildwatson dojo.Finding\nIf you're using docker:\ndocker-compose exec uwsgi ./manage.py buildwatson dojo.Finding\nUpgrading to DefectDojo Version 1.5.0 What's New:\n Updated UI with a new DefectDojo logo, default colors and CSS. Updated Product views with tabs for Product Overview, Metrics, Engagements, Endpoints, Benchmarks (ASVS), and Settings to make it easier to navigate and manage your products. New Product Information fields: Regulations, Criticality, Platform, Lifecycle, Origin, User Records, Revenue, External Audience, Internet Accessible Languages pie chart on product overview, only supported through the API and Django admin, integrates with cloc analyzer New Engagement type of CI/CD to support continual testing Engagement shortcuts and ability to import findings and auto-create an engagement Engagement labels for overdue, no tests and findings New Contextual menus throughout DefectDojo and shortcuts to new findings and critical findings Ability to merge a finding into a parent finding and either inactivate or delete the merged findings. Report improvements and styling adjustment with the default option of HTML reports SLA for remediation of severities based on finding criticality, for example critical findings remediated within 7 days. Configurable in System Settings. Engagement Auto-Close Days in System Settings. Automatically close an engagement if open past the end date. Ability to apply remediation advice based on CWE. For example XSS can be configured as a template so that it's consistent across all findings. Enabled in system settings. Finding confidence field supported from scanners. First implementation in the Burp importer. Goast importer for static analysis of Golang products Celery status check on System Settings Beta rules framework release for modifying findings on the fly DefectDojo 2.0 API with Swagger support Created and Modified fields on all major tables Various bug fixes reported on Github  Upgrading to 1.5.0 requirements:\n  Back up your database first, ideally take the backup from production and test the upgrade on a staging server.\n  Edit the settings.py file which can be found in django-DefectDojo/dojo/settings/settings.py. Copy in the rest framework configuration after the CSRF_COOKIE_SECURE = True:\nREST_FRAMEWORK = { 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework.authentication.TokenAuthentication', 'rest_framework.authentication.BasicAuthentication', ), 'DEFAULT_PERMISSION_CLASSES': ( 'rest_framework.permissions.DjangoModelPermissions', ), 'DEFAULT_RENDERER_CLASSES': ( 'rest_framework.renderers.JSONRenderer', ), 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.LimitOffsetPagination', 'PAGE_SIZE': 25 }    Navigate to: LOGIN_EXEMPT_URLS and add the following after r'^%sfinding/image/(?P\u003ctoken\u003e[^/]+)$' % URL_PREFIX:\nr'^%sfinding/image/(?P\u003ctoken\u003e[^/]+)$' % URL_PREFIX, r'^%sapi/v2/' % URL_PREFIX,  Navigate to: INSTALLED_APPS and add the following after: 'multiselectfield',:\n'multiselectfield', 'rest_framework', 'rest_framework.authtoken', 'rest_framework_swagger', 'dbbackup',  Navigate to: CELERY_TASK_IGNORE_RESULT = True and add the following after CELERY_TASK_IGNORE_RESULT line:\nCELERY_RESULT_BACKEND = 'db+sqlite:///dojo.celeryresults.sqlite'  Save your modified settings file. For reference the modified file should look like the new 1.5.0 [settings](https://github.com/DefectDojo/django-DefectDojo/blob/master/dojo/settings/settings.dist.py) file, minus the environmental configurations. As an alternative this file can be used and the enviromental configurations from you environment can be copied into this file.\nActivate your virtual environment and then upgrade the requirements:  pip install -r requirements.txt --upgrade\n Upgrade the database:\n./manage.py makemigrations ./manage.py migrate    Collect the static files (Javascript, Images, CSS):\n./manage.py collectstatic --noinput    Complete\n  Upgrading to DefectDojo Version 1.3.1 What's New:\n New importers for Contrast, Nikto and TruffleHog (finding secrets in git repos). Improved merging of findings for dynamic and static importers Markdown support for findings HTML report improvements including support of Markdown. System settings Celery status page to assist in debugging if Celery is functional.  Upgrading to 1.3.1 requires:\n pip install markdown pip install pandas ./manage.py makemigrations ./manage.py migrate ./manage.py collectstatic --noinput Complete  Upgrading to DefectDojo Version 1.2.9 What's New: New feature: Benchmarks (OWASP ASVS)\nUpgrading to 1.2.9 requires:\n ./manage.py makemigrations ./manage.py migrate ./manage.py loaddata dojo/fixtures/benchmark_type.json ./manage.py loaddata dojo/fixtures/benchmark_category.json ./manage.py loaddata dojo/fixtures/benchmark_requirement.json ./manage.py collectstatic --noinput Complete  Upgrading to DefectDojo Version 1.2.8 New feature: Product Grading (Overall Product Health) Upgrading to 1.2.8 requires:\n ./manage.py makemigrations ./manage.py migrate ./manage.py system_settings ./manage.py collectstatic --noinput pip install asteval pip install --upgrade celery Complete  Upgrading to DefectDojo Version 1.2.4 Upgrading to 1.2.4 requires:\n ./manage.py makemigrations ./manage.py migrate ./manage.py loaddata dojo/fixtures/objects_review.json  Upgrading to DefectDojo Version 1.2.3 Upgrading to 1.2.3 requires:\n ./manage.py makemigrations ./manage.py migrate ./manage.py loaddata dojo/fixtures/language_type.json Currently languages and technologies can be updated via the API or in the admin section of Django.  July 6th 2017 - New location for system settings Pull request #313 moves a number of system settings previously located in the application's settings.py to a model that can be used and changed within the web application under \"Configuration -\u003e System Settings\".\nIf you're using a custom URL_PREFIX you will need to set this in the model after upgrading by editing dojo/fixtures/system_settings.json and setting your URL prefix in the url_prefix value there. Then issue the command ./manage.py loaddata system_settings.json to load your settings into the database.\nIf you're not using a custom URL_PREFIX, after upgrading simply go to the System Settings page and review which values you want to set for each setting, as they're not automatically migrated from settings.py.\nIf you like you can then remove the following settings from settings.py to avoid confusion:\n ENABLE_DEDUPLICATION ENABLE_JIRA S_FINDING_SEVERITY_NAMING URL_PREFIX TIME_ZONE TEAM_NAME  Upgrading to DefectDojo Version 1.2.2 Upgrading to 1.2.2 requires:\n Copying settings.py to the settings/ folder. If you have supervisor scripts change DJANGO_SETTINGS_MODULE=dojo.settings.settings  Upgrading to Django 1.1.5 If you are upgrading an existing version of DefectDojo, you will need to run the following commands manually:\n  First install Yarn. Follow the instructions based on your OS: https://yarnpkg.com/lang/en/docs/install/\n  The following must be removed/commented out from settings.py: :\n'djangobower.finders.BowerFinder', From the line that contains: # where should bower install components ... To the end of the bower declarations 'justgage' )    The following needs to be updated in settings.py: :\nSTATICFILES_DIRS = ( # Put strings here, like \"/home/html/static\" or \"C:/www/django/static\". # Always use forward slashes, even on Windows. # Don't forget to use absolute paths, not relative paths. os.path.dirname(DOJO_ROOT) + \"/components/yarn_components\", )    Upgrading to Django 1.11 Pull request #300 makes DefectDojo Django 1.11 ready. A fresh install of DefectDojo can be done with the setup.bash script included - no special steps are required.\nIf you are upgrading an existing installation of DefectDojo, you will need to run the following commands manually: :\npip install django-tastypie --upgrade pip install django-tastypie-swagger --upgrade pip install django-filter --upgrade pip install django-watson --upgrade pip install django-polymorphic --upgrade pip install django --upgrade pip install pillow --upgrade ./manage.py makemigrations ./manage.py migrate  The following must be removed/commented out from settings.py: :\nTEMPLATE_DIRS TEMPLATE_DEBUG TEMPLATE_LOADERS TEMPLATE_CONTEXT_PROCESSORS  The following needs to be added to settings.py: :\nTEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ]  Once all these steps are completed your installation of DefectDojo will be running under Django 1.11\n","categories":"","description":"Release specific upgrading instructions","excerpt":"Release specific upgrading instructions","ref":"/django-DefectDojo/getting_started/upgrading/","tags":"","title":"Upgrading"},{"body":"Demo Try out the demo sever at demo.defectdojo.org\nLog in with admin / 1Defectdojo@demo#appsec. Please note that the demo is publicly accessable and regularly reset. Do not put sensitive data in the demo.\n","categories":"","description":"There is Defect Dojo demo site running the latest official released version","excerpt":"There is Defect Dojo demo site running the latest official released ‚Ä¶","ref":"/django-DefectDojo/getting_started/demo/","tags":"","title":"Demo"},{"body":"Notifications DefectDojo can inform you of different events in a variety of ways. You can be notified about things like an upcoming engagement, when someone mentions you in a comment, a scheduled report has finished generating, and more.\nThe following notification methods currently exist:\n Email Slack Microsoft Teams Alerts within DefectDojo (default)  You can set these notifications on a global scope (if you have administrator rights) or on a personal scope. For instance, an administrator might want notifications of all upcoming engagements sent to a certain Slack channel, whereas an individual user wants email notifications to be sent to the user's specified email address when a report has finished generating.\nUsers can define notifications on a product level as well, and these settings will be applied only for selected products.\nMicrosoft Teams does not provide an easy way to send messages to a personal channel. Therefore, DefectDojo can only send system scope notifications to Microsoft Teams.\nIn order to identify and notify you about things like upcoming engagements, DefectDojo runs scheduled tasks for this purpose. These tasks are scheduled and run using Celery beat, so this needs to run for those notifications to work.\nDefectDojo allows template to be used, administrator can use this feature to define which notification should be received by newly created users.\nSlack Scopes The following scopes have to be granted.\nToken The bot token has to be chosen and put in your System Settings\nMicrosoft Teams To activate notifications to Microsoft Teams, you have to:\n Configure an Incoming Webhook in a Teams channel and copy the URL of the webhook to the clipboard Activate Enable Microsoft Teams notifications in the System Settings Paste the URL of the Incoming Webhook into the field Msteams url  ","categories":"","description":"DefectDojo can inform you about changes on different channels.","excerpt":"DefectDojo can inform you about changes on different channels.","ref":"/django-DefectDojo/integrations/notifications/","tags":"","title":"Notifications"},{"body":"With the Google Sheets sync feature, DefectDojo allow the users to export all the finding details of each test into a separate Google Spreadsheet. Users can review and edit finding details via Google Spreadsheets. Also, they can add new notes to findings and edit existing notes using the Google Spreadsheet. After reviewing and updating the finding details in the Google Spreadsheet, the user can import (sync) all the changes done via the Google Spreadsheet into DefectDojo database.\nConfiguration Creating a project and a Service Account\n Go to the Service Accounts page. Create a new project for DefectDojo and select it. Click +CREATE SERVICE ACCOUNT, enter a name and description for the service account. You can use the default service account ID, or choose a different, unique one. When done click Create. The Service account permissions (optional) section that follows is not required. Click Continue. On the Grant users access to this service account screen, scroll down to the Create key section. Click +Create key. In the side panel that appears, select the format for your key as JSON Click Create. Your new public/private key pair is generated and downloaded to your machine.  Enabling the required APIs\n Go to the Google API Console. From the projects list, select the project created for DefectDojo. If the APIs \u0026 services page isn't already open, open the console left side menu and select APIs \u0026 services, and then select Library. Google Sheets API and Google Drive API should be enabled. Click the API you want to enable. If you need help finding the API, use the search field. Click ENABLE.  Configurations in DefectDojo\n  Click 'Configuration' from the left hand menu.\n  Click 'Google Sheets Sync'.\n  Fill the form.\n  Upload the downloaded json file into the Upload Credentials file field.\n  Drive Folder Id:\n  Create a folder inside the Google drive of the same Gmail account used to create the service account.\n  Get the client_email from the downloaded json file and share the created drive folder with client_email giving edit access.\n  Extract the folder id from the URL and insert it as the Drive Folder Id:\n    Tick the Enable Service check box. (Optional as this has no impact on the configuration, but you must set it to true inorder to use the feature. Service can be enabled or disabled at any point after the configuration using this check box)\n  For each field in the finding table there are two related entries in the form:\n In the drop down, select Hide if the column needs to be hidden in the Google Sheet, else select any other option based on the length of the entry that goes under the column. If the column needs to be protected in the Google Sheet, tick the check box. Otherwise leave it unchecked.      Click 'Submit'.\n  Admin has the privilege to revoke the access given to DefectDojo to access Google Sheets and Google Drive data by simply clicking the Revoke Access button.\nUsing Google Sheets Sync Feature Before a user can export a test to a Google Spreadsheet, admin must Configure Google Sheets Sync and Enable sync feature.Depending on whether a Google Spreadsheet exists for the test or not, the User interface displayed will be different.\nIf a Google Spreadsheet does not exist for the Test:\nIf a Google Spreadsheet is already created for the Test:\nAfter creating a Google Spreadsheet, users can review and edit Finding details using the Google Sheet. If any change is done in the Google Sheet users can click the Sync Google Sheet button to get those changes into DefectDojo.\n","categories":"","description":"Export finding details to Google Sheets and upload changes from Google Sheets.","excerpt":"Export finding details to Google Sheets and upload changes from Google ‚Ä¶","ref":"/django-DefectDojo/integrations/google-sheets-sync/","tags":"","title":"Google Sheets synchronisation"},{"body":"This is Burp Plugin to export findings directly to DefectDojo.\nInstallation In order for the plugin to work , you will need to have Jython set up in Burp Suite Pro . To use this plugin before it appears in the BApp Store you will need to do the following :\n Go to Extender and select the Extensions tab Click on Add , select Extension Type: to be Python and select the DefectDojoPlugin.py  Usage ","categories":"","description":"Export findings directly from Burp to DefectDojo.","excerpt":"Export findings directly from Burp to DefectDojo.","ref":"/django-DefectDojo/integrations/burp-plugin/","tags":"","title":"Defect Dojo Burp plugin"},{"body":"Import of languages for a project You can import JSON reports generated by the cloc tool via the API:\nWhen importing a file, all language information for the respective project will be deleted first and then populated with the content of the file. Please make sure to use the --json parameter when invoking the cloc command, to get the correct file format.\nDisplay The results of the import are shown on the left side of the product details page.\nThe colors are defined by entries in the table Language_Type, which has been prepopulated with data from GitHub.\nImport of language types GitHub updates its language colors from time to time, when new languages emerge. The management command\n./manage.py import_github_languages\nreads data from a JSON file hosted in https://github.com/ozh/github-colors to add new languages and update colors.\n","categories":"","description":"You can import an analysis of languages used in a project, including lines of code.","excerpt":"You can import an analysis of languages used in a project, including ‚Ä¶","ref":"/django-DefectDojo/integrations/languages/","tags":"","title":"Languages and lines of code"},{"body":"DefectDojo has protection against brute force attacks through rate limiting\nConfiguration For further information, please visit the package documentation Django Ratelimit\nEnable Rate Limiting To enable and configure rate limiting, edit the settings (see Configuration) and edit/replace the following information:\nDD_RATE_LIMITER_ENABLED=(bool, True), DD_RATE_LIMITER_RATE=(str, '5/m'), DD_RATE_LIMITER_BLOCK=(bool, True), DD_RATE_LIMITER_ACCOUNT_LOCKOUT=(bool, True), Rate Limit The frequency at which the request will be limited can be set to\n seconds - 1s minutes - 5m hours - 100h days - 2400d  Extended configuration can be found here\nBlock Requests By default, rate limiting is set to record offenses, but does not actually block requests and enforce the limit.\nSetting DD_RATE_LIMITER_BLOCK will block all incoming requests at the configured frequncy once that frequency has been exceeded.\nAccount Lockout In the event of a brute force attack, a users credentials could potentially be comprimised.\nIn an attempt to circumvent that event, setting DD_RATE_LIMITER_ACCOUNT_LOCKOUT will force a user to reset their password upon the next attempted login.\nMulti-Process Behavior When using configurations with multiple uwsgi processes, the rate limiting package uses the default cache that is memory based and local to a process.\nExtra Configuation For further information, please visit the package documentation Django Ratelimit\n","categories":"","description":"Configurable rate limiting on the login page to mitigate brute force attacks","excerpt":"Configurable rate limiting on the login page to mitigate brute force ‚Ä¶","ref":"/django-DefectDojo/integrations/rate_limiting/","tags":"","title":"Rate Limiting"},{"body":"Export Findings Pages that show a list of findings or a list of engagements have a CSV and Excel Export functionality in the top right dropdown menu.\nThe list of engagements can be exported as CSV/Excel.\n","categories":"","description":"DefectDojo has the ability to export findings.","excerpt":"DefectDojo has the ability to export findings.","ref":"/django-DefectDojo/integrations/exporting/","tags":"","title":"Exporting"},{"body":"About DefectDojo What is DefectDojo? DefectDojo is a security tool that automates application security vulnerability management. DefectDojo streamlines the application security testing process by offering features such as importing third party security findings, merging and de-duping, integration with Jira, templating, report generation and security metrics.\nWhat does DefectDojo do? While traceability and metrics are the ultimate end goal, DefectDojo is a bug tracker at its core. Taking advantage of DefectDojo's Product:Engagement model, enables traceability among multiple projects and test cycles, and allows for fine-grained reporting.\nHow does DefectDojo work?  Getting started will tell you how to install and configure DefectDojo. Usage shows how to use DefectDojo to manage vulnerabilities. A lot of integrations help to fit DefectDojo in your environment. Contributing gives insights how you can help to make DefectDojo even better.  Where to find DefectDojo? The code is open source, and available on GitHub.\nA running example is available on the demo server, using the credentials admin / defectdojo@demo#appsec. Note: The demo server is refreshed regularly and provisioned with some sample data.\nYou can also find videos of demos on our YouTube channel.\n","categories":"","description":"","excerpt":"About DefectDojo What is DefectDojo? DefectDojo is a security tool ‚Ä¶","ref":"/django-DefectDojo/","tags":"","title":"DefectDojo's Documentation"}]